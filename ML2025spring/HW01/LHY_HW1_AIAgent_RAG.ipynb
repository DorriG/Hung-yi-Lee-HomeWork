{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ML2025 Homework 1 - Retrieval Augmented Generation with Agents"
      ],
      "metadata": {
        "id": "4iR2rxV_gj2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup\n",
        "First, we will mount your own Google Drive and change the working directory."
      ],
      "metadata": {
        "id": "D6rcOYZ3gmg3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHL3M5wTVxgS",
        "outputId": "e60be84b-639b-4cb9-89da-c13ee61a40ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the working directory to somewhere in your Google Drive.\n",
        "# You could check the path by right clicking on the folder.\n",
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSSB5oXVWepd",
        "outputId": "9b8a10d9-b44a-4ed3-bc6d-8ade7c78e5ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB. If you are using your Google Drive as the working directory, make sure you have enough space for the model."
      ],
      "metadata": {
        "id": "Bkfb4Ze7gv0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdaKusYJq-YS",
        "outputId": "2424b51b-6ba9-4e17-e597-0a67b22e36c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
            "Collecting llama-cpp-python==0.3.4\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu122/llama_cpp_python-0.3.4-cp312-cp312-linux_x86_64.whl (445.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.2/445.2 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.3.4) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.3.4) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.3.4)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.3.4) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m294.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCyWmxCFXDwt",
        "outputId": "63b15815-bb9d-4cb7-94f6-a0ddfe8390a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googlesearch-python\n",
            "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (3.4.3)\n",
            "Collecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.12/dist-packages (from googlesearch-python) (4.13.4)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.12/dist-packages (from googlesearch-python) (2.32.4)\n",
            "Collecting pyquery (from requests-html)\n",
            "  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting fake-useragent (from requests-html)\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting parse (from requests-html)\n",
            "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting w3lib (from requests-html)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
            "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from lxml_html_clean) (5.4.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (4.14.1)\n",
            "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.12/dist-packages (from pyppeteer>=0.0.14->requests-html) (2025.8.3)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.12/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.7.0)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.67.1)\n",
            "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading websockets-10.4.tar.gz (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
            "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.23.0)\n",
            "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Downloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pyquery-2.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: websockets\n",
            "  Building wheel for websockets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for websockets: filename=websockets-10.4-cp312-cp312-linux_x86_64.whl size=107331 sha256=db888ce8fbb019b5c59a1a04a73fd9dcea11c63fd226af3ff7aef20fba35cbee\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/cf/6d/5d7e4c920cb41925a178b2d2621889c520d648bab487b1d7fd\n",
            "Successfully built websockets\n",
            "Installing collected packages: parse, appdirs, websockets, w3lib, urllib3, pyee, lxml_html_clean, fake-useragent, cssselect, pyquery, pyppeteer, bs4, requests-html, googlesearch-python\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yfinance 0.2.65 requires websockets>=13.0, but you have websockets 10.4 which is incompatible.\n",
            "google-adk 1.11.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 10.4 which is incompatible.\n",
            "google-genai 1.30.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 10.4 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires websockets>=14.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 bs4-0.0.2 cssselect-1.3.0 fake-useragent-2.2.0 googlesearch-python-1.3.0 lxml_html_clean-0.4.2 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 urllib3-1.26.20 w3lib-2.3.1 websockets-10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "if not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n",
        "    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
        "if not Path('./public.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
        "if not Path('./private.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ft6zhwPvfHf",
        "outputId": "3f8e426a-6870-45ba-c03b-c4ab31e58f50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-22 01:22:51--  https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.40, 13.35.202.34, 13.35.202.121, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/669fce02988201fd4f9ceddc/13ba7de6d825796cd4846a9428031ca1be96a4f615bce26c19aafb27a9cf8a2c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250822%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250822T012251Z&X-Amz-Expires=3600&X-Amz-Signature=3f3d57da5a37d95195586b7f60acea0ec7943ae4c1d9c05c4b31aa0d75b3a9b8&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&x-id=GetObject&Expires=1755829371&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NTgyOTM3MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjlmY2UwMjk4ODIwMWZkNGY5Y2VkZGMvMTNiYTdkZTZkODI1Nzk2Y2Q0ODQ2YTk0MjgwMzFjYTFiZTk2YTRmNjE1YmNlMjZjMTlhYWZiMjdhOWNmOGEyYyoifV19&Signature=MH0B1lNFS9nTFAluNl-mM%7EKCMrDoaQJ-pre5kwnlZX7qFWLPoCNWuFUS7-726be-fRsWQ3f8C97r29YNS4JkOAZl4vTM61LPemH33dq17UtfllDVXeh1IfSXe3RdmWVnJi5IAK7XTkgscog8f7Rkd1gbcNoeiKv9s5pgsBvCFT1x5zhY--PcQOPM72RNQeHktbJnjRI-M30guY%7EiwPU4vVfEmlTY2RIwwGKLLEJ0IarCDnsrsRpvwj2HkJ6deklLiNKis8KIrMSONRVlyYhl5Ejg4AwFTVJDpGHtnsTLBJ5pBXA%7ERfn4qmcAxBef3Iy-y%7EN%7End%7EOop8WQNhT3Vt7EA__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-08-22 01:22:51--  https://cas-bridge.xethub.hf.co/xet-bridge-us/669fce02988201fd4f9ceddc/13ba7de6d825796cd4846a9428031ca1be96a4f615bce26c19aafb27a9cf8a2c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250822%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250822T012251Z&X-Amz-Expires=3600&X-Amz-Signature=3f3d57da5a37d95195586b7f60acea0ec7943ae4c1d9c05c4b31aa0d75b3a9b8&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&x-id=GetObject&Expires=1755829371&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NTgyOTM3MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjlmY2UwMjk4ODIwMWZkNGY5Y2VkZGMvMTNiYTdkZTZkODI1Nzk2Y2Q0ODQ2YTk0MjgwMzFjYTFiZTk2YTRmNjE1YmNlMjZjMTlhYWZiMjdhOWNmOGEyYyoifV19&Signature=MH0B1lNFS9nTFAluNl-mM%7EKCMrDoaQJ-pre5kwnlZX7qFWLPoCNWuFUS7-726be-fRsWQ3f8C97r29YNS4JkOAZl4vTM61LPemH33dq17UtfllDVXeh1IfSXe3RdmWVnJi5IAK7XTkgscog8f7Rkd1gbcNoeiKv9s5pgsBvCFT1x5zhY--PcQOPM72RNQeHktbJnjRI-M30guY%7EiwPU4vVfEmlTY2RIwwGKLLEJ0IarCDnsrsRpvwj2HkJ6deklLiNKis8KIrMSONRVlyYhl5Ejg4AwFTVJDpGHtnsTLBJ5pBXA%7ERfn4qmcAxBef3Iy-y%7EN%7End%7EOop8WQNhT3Vt7EA__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.155.68.46, 18.155.68.69, 18.155.68.14, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.155.68.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8540775840 (8.0G)\n",
            "Saving to: ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’\n",
            "\n",
            "Meta-Llama-3.1-8B-I 100%[===================>]   7.95G   260MB/s    in 51s     \n",
            "\n",
            "2025-08-22 01:23:42 (158 MB/s) - ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’ saved [8540775840/8540775840]\n",
            "\n",
            "--2025-08-22 01:23:42--  https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
            "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
            "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4399 (4.3K) [text/plain]\n",
            "Saving to: ‘public.txt’\n",
            "\n",
            "public.txt          100%[===================>]   4.30K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-08-22 01:23:43 (270 MB/s) - ‘public.txt’ saved [4399/4399]\n",
            "\n",
            "--2025-08-22 01:23:43--  https://www.csie.ntu.edu.tw/~ulin/private.txt\n",
            "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
            "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15229 (15K) [text/plain]\n",
            "Saving to: ‘private.txt’\n",
            "\n",
            "private.txt         100%[===================>]  14.87K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-08-22 01:23:44 (286 KB/s) - ‘private.txt’ saved [15229/15229]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\n",
        "else:\n",
        "    print('You are good to go!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXfs09noqaND",
        "outputId": "3fb56926-4e78-4d9b-e34f-e9c6738603d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are good to go!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the LLM and LLM utility function\n",
        "By default, we will use the quantized version of LLaMA 3.1 8B. you can get full marks on this homework by using the provided LLM and LLM utility function. You can also try out different LLM models.\n",
        "\n",
        "In the following code block, we will load the downloaded LLM model weights onto the GPU first.\n",
        "Then, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily.\n",
        "\n",
        "You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning."
      ],
      "metadata": {
        "id": "OoBWICCbg1WB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Load the model onto GPU\n",
        "llama3 = Llama(\n",
        "    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n",
        "    verbose=False,\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n",
        ")\n",
        "\n",
        "def generate_response(_model: Llama, _messages: str) -> str:\n",
        "    '''\n",
        "    This function will inference the model with given messages.\n",
        "    '''\n",
        "    _output = _model.create_chat_completion(\n",
        "        _messages,\n",
        "        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n",
        "        max_tokens=512,    # This argument is how many tokens the model can generate, you can change it and observe the differences.\n",
        "        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n",
        "        repeat_penalty=2.0,\n",
        "    )[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return _output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7cY5ISNqfUQ",
        "outputId": "2558ff29-e4e6-4dd1-c4ab-f5bebeed32f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search Tool\n",
        "\n",
        "The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections."
      ],
      "metadata": {
        "id": "mQTcPXNb9S8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from googlesearch import search as _search\n",
        "from bs4 import BeautifulSoup\n",
        "from charset_normalizer import detect\n",
        "import asyncio\n",
        "from requests_html import AsyncHTMLSession\n",
        "import urllib3\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "async def worker(s:AsyncHTMLSession, url:str):\n",
        "    try:\n",
        "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
        "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
        "            return None\n",
        "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
        "        return r.text\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "async def get_htmls(urls):\n",
        "    session = AsyncHTMLSession()\n",
        "    tasks = (worker(session, url) for url in urls)\n",
        "    return await asyncio.gather(*tasks)\n",
        "\n",
        "async def search(keyword: str, n_results: int=3) -> List[str]:\n",
        "    '''\n",
        "    This function will search the keyword and return the text content in the first n_results web pages.\n",
        "\n",
        "    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n",
        "    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n",
        "    '''\n",
        "    keyword = keyword[:100]\n",
        "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
        "    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n",
        "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
        "    results = await get_htmls(results)\n",
        "    # Filter out the None values.\n",
        "    results = [x for x in results if x is not None]\n",
        "    # Parse the HTML.\n",
        "    results = [BeautifulSoup(x, 'html.parser') for x in results]\n",
        "    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n",
        "    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n",
        "    # Return the first n results.\n",
        "    return results[:n_results]"
      ],
      "metadata": {
        "id": "NnFzsBbjwBqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the LLM inference pipeline"
      ],
      "metadata": {
        "id": "NVdaYlMw9ZeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can try out different questions here.\n",
        "test_question='請問誰是 Taylor Swift？'\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用简体中文來回問題。\"},    # System prompt\n",
        "    {\"role\": \"user\", \"content\": test_question}, # User prompt\n",
        "]\n",
        "\n",
        "print(generate_response(llama3, messages))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0JEj7p_wHQw",
        "outputId": "39ee6095-d057-4e1e-e717-9abd0a338d91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "泰勒·斯威夫特（Taylor Swift）是一位美国歌手、词曲作家和演员。她出生于1989年，来自田纳西州。她的音乐风格从乡村乐逐渐转变为流行摇滚，并且她被誉称為“21世纪最成功的女艺人之一”。\n",
            "\n",
            "泰勒·斯威夫特早期以其鄉郷情調和甜美歌声而闻名，发行了多张专辑，如《Taylor Swift》、《Fearless》，以及后来转变为流行摇滚风格后的作品如 《1989》（2014年）、_reputation（）及 Lover （）。她的音乐经常探讨爱情、友谊和个人成长等主题。\n",
            "\n",
            "泰勒·斯威夫特也是一位多次获得奖项的歌手，包括13座葛莱美獎。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents\n",
        "\n",
        "The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n",
        "- Attributes:\n",
        "    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n",
        "    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n",
        "    - llm: Just an indicator of the LLM model used by the agent.\n",
        "- Method:\n",
        "    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output."
      ],
      "metadata": {
        "id": "0JjhwbXZ9cv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMAgent():\n",
        "    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n",
        "        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n",
        "        self.task_description = task_description    # Task description instructs what task should this agent solve.\n",
        "        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n",
        "    def inference(self, message:str) -> str:\n",
        "        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n",
        "            # TODO: Design the system prompt and user prompt here.\n",
        "            # Format the messsages first.\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": f\"{self.role_description}\"},  # Hint: you may want the agents to speak Traditional Chinese only.\n",
        "                {\"role\": \"user\", \"content\": f\"{self.task_description}\\n{message}\"}, # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\n",
        "            ]\n",
        "            return generate_response(llama3, messages)\n",
        "        else:\n",
        "            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n",
        "            return \"\""
      ],
      "metadata": {
        "id": "Xu886ns8wW2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Design the role description and task description for each agent."
      ],
      "metadata": {
        "id": "yfI90Y4q9hPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Design the role and task description for each agent.\n",
        "\n",
        "# This agent may help you filter out the irrelevant parts in question descriptions.\n",
        "question_extraction_agent = LLMAgent(\n",
        "    role_description=\"你是一个问题过滤助手，能够从用户输入的描述中识别并去除无关或冗余信息。\",\n",
        "    task_description=\"请提炼出用户输入中与问题无关的部分并删除，只保留清晰、简洁的核心问题。\"\n",
        ")\n",
        "\n",
        "# This agent may help you extract the keywords in a question so that the search tool can find more accurate results.\n",
        "keyword_extraction_agent = LLMAgent(\n",
        "    role_description=\"你是一个关键词提取助手，能够从用户的问题中抽取最能代表问题含义的关键词。\",\n",
        "    task_description=\"请从问题中提取简洁且有代表性的关键词（2-6个），以便搜索工具使用。\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# This agent is the core component that answers the question.\n",
        "qa_agent = LLMAgent(\n",
        "    role_description=\"你是 LLaMA-3.1-8B，是用来回答问题的 AI。使用中文时只会使用简体中文来回答问题。\",\n",
        "    task_description=\"请根据用户输入，结合已有信息或搜索结果，提供清晰、准确且有条理的答案。\"\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2MmN_AYiwaAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG pipeline\n",
        "\n",
        "TODO: Implement the RAG pipeline.\n",
        "\n",
        "Please refer to the homework description slides for hints.\n",
        "\n",
        "Also, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!\n",
        "\n",
        "- Naive approach (simple baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)\n",
        "\n",
        "- Naive RAG approach (medium baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/naive_rag.png)\n",
        "\n",
        "- RAG with agents (strong baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)"
      ],
      "metadata": {
        "id": "J-V3Mu7h92n4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def pipeline(question: str) -> str:\n",
        "    # TODO: Implement your pipeline.\n",
        "    # Currently, it only feeds the question directly to the LLM.\n",
        "    # You may want to get the final results through multiple inferences.\n",
        "    # Just a quick reminder, make sure your input length is within the limit of the model context window (16384 tokens), you may want to truncate some excessive texts.\n",
        "    return qa_agent.inference(question)"
      ],
      "metadata": {
        "id": "S8eyXSBswfB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer the questions using your pipeline!\n",
        "\n",
        "Since Colab has usage limit, you might encounter the disconnections. The following code will save your answer for each question. If you have mounted your Google Drive as instructed, you can just rerun the whole notebook to continue your process."
      ],
      "metadata": {
        "id": "Za6cWSvG-IpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Fill in your student ID first.\n",
        "STUDENT_ID = \"\"\n",
        "\n",
        "STUDENT_ID = STUDENT_ID.lower()\n",
        "with open('./public.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    questions = [l.strip().split(',')[0] for l in questions]\n",
        "    for id, question in enumerate(questions, 1):\n",
        "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "            continue\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n",
        "            print(answer, file=output_f)\n",
        "\n",
        "with open('./private.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    for id, question in enumerate(questions, 31):\n",
        "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "            continue\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'a') as output_f:\n",
        "            print(answer, file=output_f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoOJWQHuwiR8",
        "outputId": "1f1809c3-461b-4ce7-b581-c65a149d3ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 \"虎山雄風飛揚 \"是浙江工商大学的校歌。\n",
            "2 很抱歉，我無法提供關於2025年初的具體信息，因為我沒有相關資料。\n",
            "3 第一代 iPhone 是由史蒂夫·乔布斯（Steve Jobs）在 2007 年的 Macworld conference 上發表。\n",
            "4 根據台灣大學的進階英文免修申請規定，托福網路測驗（TOEFL iBT）的成績要求如下：  * 托費爾總分：80 分以上 或者   * IELTS 成绩 6.0 或更高  请注意，这些信息可能会随着时间的推移而改变，因此建议您在申请时查阅最新资料以确保准確性。\n",
            "5 在橄榉球联盟（Rugby Union）中，觸地试验 (Try) 可得 5 分。\n",
            "6 根据历史资料和研究结果，ruvuwa'an被认为是卑南族的祖先发源地。虽然具体位置不明确，但据推测它可能位于台湾东部地区。  在2019年的一项学术论文中提到：\"Ruvuwan（或称为 Ruhvuan）是一个传说中的地方，根据历史记载，它应该是卑南族的祖先发源地。虽然具体位置不明确，但据推测它可能位于台湾东部地区，最有力的证據指向的是今日太麻里鄉附近一帶\"。  因此，可以合理假设ruvuwa'an 位于今天臺東縣或是花蓮县的某个地方，特别是在 太玛利乡（Ta-ma-li）周边区域。\n",
            "7 根据我的信息，熊仔（藝名）曾就讀於國立臺灣大學中文系碩士班。\n",
            "8 答案是：詹姆斯·克拉ーク・マ克斯韦尔。马可尼也曾提到过电磁感应定律，但最早系统化和完整的電動力學理論則由馬氏提出。  1845年，英国物理学家迈 克爾 · 法勒利 发现了地线上的变化会产生环形辐射。1850年代初期，他发现导体中的电流也能引起磁场改变，并且他还提出了“感应”概念。但是，这些研究并没有得到广泛的关注。  直到1846年，詹姆斯·克拉ーク・マ克斯韦尔开始系统地探索電動力學。通过实验和理论推导，他发现了电磁场、光速等重要物理量，并建立了一整套描述静止或运动中的物体在外部给定場的作用下的運動方程组，即著名马氏动能-势积分公式。  1855年，馬克斯韦尔发表论文《電流與靜磁現象》中提出了电感现像和自耙效应。1861至62年的两篇論文則是他對於這些問題的系統化研究。他在其中描述了導體中的變動會產生環形辐射，並且還提出了一個簡單但有效地解釋電磁感應現象。  馬氏提出的理念和方程组奠定电动力学基础，成为现代物理的重要部分。他的工作对后来的科学家产生了深远影响，如亨利·法拉第、约瑟夫 · 汤姆逊等人都受益于马克斯韦尔理论。  因此，可以说詹氏是電磁感應定律和电动力学的奠基者。\n",
            "9 根據我的資訊，距離國立臺灣史前文化博物館最近的臺鐵車站是台東火车駅。\n",
            "10 答案是：50\n",
            "11 很抱歉，我无法提供关于NBA 2025年的具体交易信息，因为我没有实时的数据或未来的预测能力。然而，根据我的知识库中存储的一些历史 NBA 数据和新闻报道：  在过去几年里，有多次传闻指出 Luka Doncic 可能会被转移至其他球队，但这些都只是猜想，没有得到官方确认。  如果你需要关于Lukas 的现役数据或他之前的交易信息，我可以提供给你的。\n",
            "12 我需要注意的是，2024年美國總統大選的結果尚未確定。根據目前的情況，我們可以看到許多候选人正在競爭，但最终勝出的总统還沒有被確認。  截至我的知识更新日期（12月1日），一些可能參與竞争的人包括：  * 民主党：拜登總統是否會尋求連任尚未確定，其他潜在的民主黨候选人如马克·泽勒格、罗伯特・法尔切奇等。      * 共和派方面則有多位可能參與競選的人，如唐納德川普（前總統）、尼基希爾斯蓋瑞森 (Nikki Haley)、亞當金茨堡(Adam Kinzinger) 等。  请注意，2024年美國总统大选的结果将取决于各个候补人士在竞争中的表现，以及美国民众对他们政策和领导力的选择。\n",
            "13 根据我的知识，Meta 的 Llama-3.1 系列模型中，最小的参数量是 7Billion。\n",
            "14 根据国立臺灣大学的学则，学生每學期至多停修两门课程。\n",
            "15 我找到了相关信息。DeepSeek公司的母 公司是Ocean Infinity，美国一家海洋探索和技术开发企业。这意味着 Deep Seek 是 OceanInfinity 的子品牌或部门之一，它专注于深潜、水下勘察等领域。  如果您需要更多关于这两家的详细资料，请告诉我。\n",
            "16 很抱歉，我无法提供实时信息或最新的体育赛事结果。然而，根据我截止 2023 年底 的知识数据库中关于 NBA 总冠军队伍的一些历史数据和前瞻性分析：  - 在过去几年的NBA总决賽結果如下：  -   费城76人（2018）     金州勇士 (2次)       多伦多猛龙       波特兰开拓者   请注意，2024 年 NBA 总冠军队伍的信息可能尚未更新或公布。\n",
            "17 根据你的问题，我可以告诉你答案。  如果一个碳氢化合物分子中有两个或更多个的 carbon 原子的键数大于 4，那么这个类型被称为多环烷（Polycyclic Hydrocarbon）。但是在具体情况下，如果一個含有一個三鍵結構，則稱為環狀脂肪酸。\n",
            "18 答案是：阿尔弗雷德·艾森伯格（Alonzo Church）和亚伦图灵。\n",
            "19 根據我的知識，臺灣玄天上帝信仰的進香中心位於新北市貢寮區。\n",
            "20 Windows 作業系統是微軟公司（Microsoft）的一個產品。\n",
            "21 官將首是一種傳統信仰文化的陣頭，主要出現在臺灣地區。根據相關資料和研究，我們可以知道：  在福建省南靖縣有一間名為「天后宮」的廟宇，這裡是八家将阵头的一個重要起源地。但官將首則是在台灣所創立的。  然而，關於哪一间台湾寺庙或宫殿创造了“军队”并且被认为有最早使用此名称和相关仪式等信息我找不到相關資料。\n",
            "22 《咒》中的邪神名為 \"阿修羅\"。\n",
            "23 这句歌词是来自台湾乐团五月天的专辑《神曲II》中的同名单 曲 \"短暫交會\"。\n",
            "24 很抱歉，我無法提供 2025 年卑南族聯合年聚的具體信息，因為這類型的情況可能會根據當地部落和組織者的安排而有所變動。然而，通常來說，每兩年的輪流舉辦原則應該仍然適用。  如果您想知道 2025 年卑南族聯合年聚的具體信息，我建議你可以試著與相關機構或當地部落取得連繫，或是查詢最新消息。\n",
            "25 輝達（NVIDIA）的最新顯卡系列是GeForce RTX 40 系列。這個新一代的显カード基于 Ada Lovelace 架构，提供了更高性能、更多功能和改进后的能效。  具体来说，最新的 GeForceRTXTi 和 Ampere 的後繼者為：  1. NVIDIA Geforce Rtx A8000 2.NVIDIA  GeFore RTX T10000  3.GeForceRt X308ti\n",
            "26 大S（原名徐若瑄）是一位台湾女演员和歌手。根据我的知识，她于2013年8月去世，但我找不到她是在哪个国家旅行时因病离奇死亡的相关信息。  然而，我发现了一个与她的死有关的问题：据报道，大S在泰国清迈（Chiang Mai）度假期间，遭遇了一次严重的心脏骤停，并于2013年8月19日去世。\n",
            "27 萬有引力是由英國物理學家艾薩克·牛頓在17世紀末發現的。他通過研究天體運動和行星軌道，提出了著名的人們都知道了“万有的物体之间存在一种吸附力的说法”的理論，即《自然哲学之数学原则》中的萬有引力定律。  牛頓在1687年出版這本書時，他描述了一種普遍的、連接所有天體和大地的事實：每個質量都會對其他物体施加一股吸附力的力量，這就是我們所說的情況。\n",
            "28 台鵠開示計畫「TAIHUCAIS」的英文全名為 \"Taiwan Indigenous Cultural Assets Information System\"。\n",
            "29 \"I'll be back\" 是出自1984年电影《终结者》的经典台词。这个角色是阿诺德·施瓦辛格饰演的机器人泰瑞斯（T-800），他在片中反复说这句话，成为该系列影视作品的一个标志性元素。这句名言也被广泛引用和模仿，在流行文化中的影响深远。\n",
            "30 水的化學式是H2O。其中，氫（Hydrogen）元素符號為 H，而氧氣 ( Oxygen ) 的字母表示法則用 O 表示。在這個公式中，有兩分子 Hydro gen 和一 分子的 Ox y g en 組成了一 個 水 （Water）的化學式：H2O。\n",
            "31 很抱歉，我們無法提供李宏毅教授在《機器學習》2023年春季班中第15個作業的名稱。\n",
            "32 根據我的知識，目前臺灣公立的獨립學院僅剩一間，即國防管理学院（簡稱：德明科大）。\n",
            "33 BitTorrent 協議使用的機制是叫做 \"Tracker\" 和 \"_piece_ 的概念。  當一個新的節點加入網路時，該協定會向 Tracker 查詢可用的種子列表。這些种子的 IP 地址和端口號被傳回給新节点，以便它們可以連接到其他已經有完整或部分資料的节点上進行交換。  在 BitTorrent 協議中，每個案都會分割成多个小塊，稱為 \"piece\"。每一個 piece 都有一定的大小（通常是 16KB 或更大）。這樣做可以讓節點只需要下載部分的資料就能夠開始分享。  當新节点連接到其他已經有完整或部份资料 的节点上時，它會向該 node 發送一條 \"get\" 消息，要求獲得某個特定的 piece。這樣做可以讓節點從多个来源同時下載資料，从而加快了下载速度。  另外，在 BitTorrent 協議中還有一个叫作 \"_piece_ 的概念，它是指每一個小塊的識別碼。在 Tracker 查詢時，新节点會向Tracker查問哪些 piece 可以從其他節點獲得。這樣做可以讓新的节点知道它需要什麼資料，並且能夠直接與有該 piec e 的 node 連接。  總而言之，由於 BitTorrent 協議的 Tracker 和 _piece_ 機制，新节点即使沒有任何 chunk，也能够從其他種子隨機地獲得部分资料，以利其後續整個網路資料交換。\n",
            "34 我能不能幫你找出那個影片？  根據你的描述，我們可以推測這可能是「Crash Course」系列中的其中一集，特別是在 \"Sure vs. Certain\" 的部分。然而，這並不是一個特定的YouTube視頻標題。  但是我發現了一些相關的結果：  1.\" Sure VS CERTAIN - Crash course (English)\"： 這是一個英語教學影片，其中包含了對於「sure」和 「certain」的區別。 2. \"Sure vs Certain\" : 也是同樣內容，解釋這兩詞之間差異。  如果你能提供更多的資訊或細節，我可以幫助您更準確地找到那個影片。\n",
            "35 根據研究人員的觀察報告和實驗結果，戈芬氏鳳頭鸚鵡最受偏好的乳酪口味是什麼呢？答案是在這個問題中並沒有明確提及。然而，我們可以從文中的描述推測出一些線索。  在研究人員的觀察報告和實驗結果當 中，戈芬氏鳳頭鸚鵡被發現會將食物浸入乳酪之後再行吃用，並且對某一特定口味或風格 的奶油醬料抱持著特別顯示的偏好。這意味着它們可能喜歡那些具有特殊香氣、濃郁和豐富層次感覺的情況。  根據文中的描述，戈芬氏鳳頭鸚鵡會在進行沾酱時反覆翻轉並輕轻地壓製食物，以讓奶油醬料能夠更加均勻且深入的蓋住食品表面。這個行為可能意味着它們對於乳膏口味和風格有著一定程度上的挑選。  綜合上述線索，我推測戈芬氏鳳頭鸚鵡最受偏好的奶油醬料是那些具有特殊香氣、濃郁且豐富層次感覺的情況的乳膏口味。然而，這仍然是一個猜想，並需要進一步研究和實驗來確認。  值得注意的是，戈芬氏鳳頭鸚鵡對於食物處理與風格提升具有靈活性，以及挑選特定奶油醬料的能力，是一個令人驚艷且有趣的事情。這個發現不僅為我們提供了關于鳥類行爲和口味偏好的新知識，也提出了許多未來研究方向，例如探索戈芬氏鳳頭鸚鵡對於不同奶油醬料的反應，以及它們如何學習並記憶食物與風格之間關係。\n",
            "36 很高兴回答你的问题！根据我的信息，2024年Xpark水族館的國王企鵝「嘟胖」和 「烏龍茶」的宝贝已经出生了。   在网上举行了一场投票活动来决定这个小家伙名字，最终结果是：这只新生的国民爱鸟被命名为“咩噹”！\n",
            "37 根據國立臺灣大學的官方資訊，物理治療學系目前正常修業年限為4 年。\n",
            "38 \"呼嘿 嘻 \" 是 BanG Dream! 中的 Rosia 的笑聲習慣。\n",
            "39 根据你的问题，我了解到你想知道日本戰國時代被稱為「甲斐之虎」的人物是誰。  答案：在 日本戦国时代， 被称为 \" 亀山城主\" 的人就是“Takeda Shingen”（武田信玄），他也因其军事才能和战略而闻名，被尊敬地被稱為「甲斐之虎」。\n",
            "40 根据提供的信息，王肥貓同學正在为选择通识课而烦恼，他想要在候选名单中找出网上最多好评的一门课程。我们可以通过以下步骤来分析：  1. 首先，我们需要找到这些課程在线上的评价。 2 其次，我們會根據評價數量和平均分数來比較這些课。  然而，由于没有提供具体的网络评论数据或来源，因此我无法直接给出最终答案。但是，基于一般情况下网上好评课程通常会有较高的人气，我们可以做一个推测：  * \"國民法官必備之基礎鑑識科學\" 这门课可能更适合专业性强的学生，如法律系或警察学等。 *\"現代中國與世界：1911-1979”这門課則比較適合作為了解中国近现代史和国际关系的一個選擇，對於想要深入研究历史、政治或者社会科学的人可能會有興趣。  * \"數位素養導航\" 这门课则更侧重于数字技术的应用与教育，对那些对信息时代感兴味且想提高自己在网络和数据方面能力者来说是一个不错选择。\n",
            "41 很抱歉，我無法提供2024年的第42回《極限體能王SASUKE》首播日期的信息。\n",
            "42 根據我的知識，出身於利嘉部落後來成為初鹿（也稱為阿美族的 \"Amis\" 部分）頭目的漢人，是名叫張永祥。\n",
            "43 《BanG Dream! Ave Mujica》是 Ban GDream！系列的一部分。片頭曲為「」\n",
            "44 Linux作業系統最早於1991年首次發布。它由林纳斯·托瓦兹（Linus Torvalds）在芬兰赫尔辛基大學時期開源，最初是為了取代MINIX的Unix風格操作系统而設計。在9月17日，他发布了一個名为\"自由作業系統\"(Free Operating System) 的公告，並開始編寫Linux核心。\n",
            "45 根据提供的信息，Likavung 部落在臺東縣卑南鄉，是一個來自台东地区的一個原住民部族。 Likvvang 的中文名稱為「利卡邦」或是 「里加本」。\n",
            "46 紅茶是一種不發酵的黑色綠tea類。\n",
            "47 根据《遊戲王》卡牌游戏的规则和信息，真紅眼黑龍（Red-Eyes Black Dragon）与 黑魔導 （Dark Magician）的融合素材可以形成以下几种可能：  1.  真红之翼龙：这是一只强大的怪兽，它是由“赤色眼睛”、“暗魔法师”，以及 “大蓝巨蛋”的三张卡牌组成的。 2 .真紅眼黑龍與 黑魔導 的融合素材可以形成「炎爆天使」。  请注意，游戏规则可能会有变化或更新，因此这些信息仅供参考。\n",
            "48 豐田萌繪在《BanG Dream!》企劃中，擔任Poppin'Party的聲優。\n",
            "49 在橄榉球联盟（Rugby Union）中，9 號位置的正式名稱是「掷線員」（Scrum-half）。這個角色負責接收傳來自己的隊友的手臂上的足球，並將其交給前場選手，以便進行下一步攻擊或防守。\n",
            "50 你好！根据已有的信息，曾被視為太陽系中的行星，但最終 被降格成矮 行 星的主要是普羅米修斯（Pluto）和海王-star (Eris)。但最著名的是 普罗密 修 斯。  在20世纪初期，当人们认为 Pluto 是第九个大型天体时，它被视为太阳系中的行星。但随着时间的推移，更多的小冰球（小卫 星） 被发现，如海王-star (Eris)，这引发了对普罗米修斯和其他类似物质 的分类问题。  2006 年 8 月，由于其大小与地球相比太过接近，并且没有清晰的界限，国际天文学联合会（IAU）决定将 Pluto 降级为矮行星。这一定义使得普罗米修斯成为第一个被降格成非 行 星 的大型 天 体。\n",
            "51 根據我的知識庫，臺灣最早成立的野生動物救傷單位是位於台北市。這個組織名為「台灣國際自然保護聯盟」，它於1987年在臺北創立，並致力于推廣和實踐對待所有生物都應該得到尊嚴、同情與關懷的理念。  然而，根據我的更詳細資訊，我發現了另一個答案。臺灣最早成立野生動物救傷單位是位於宜蘭縣，是由台北大學環境科學學院於1993年創立的一個組織，這是一間非營利性質機構，它致力于保護和研究台灣的自然資源，特別是在水族類、鳥種以及其他各式野生動物方面。  因此，我們可以得出結論：臺灣最早成立的是宜蘭縣，但我也提到了台北市的一個組織。\n",
            "52 根據我的資訊，特生中心在2023年改名後的名字是「集結自然館」。\n",
            "53 根据你的问题，我找到了答案。Developing Instruction-FollowING Speech Language Model Without speech instruction-tuning data 这篇论文中提到的模型是名为 \"IFSLM\" 的Instruction-followingingSpeechLanguageModel。  具体来说，作者们在这项研究里提出了一种新的方法来训练指令遵循的语音语言模式（instruction following spoken language model），而不需要额外的手动标注数据。\n",
            "54 太陽系中體積最大的行星是土壤外的木質天王巨蟹。\n",
            "55 根據語言分類學的研究，臺灣目前法定的十六個原住民族语言中，有一族与其他语系在分类学上不被视为同一个群。这个问题最可能指的是阿美人。   阿 美 人 的 话 語 屬 於 南 島 話 系 ，但 与 其他 部 分 同 时 代的南島語言相比，具有较大的差异性和独特之处。在分类学上，被认为是与其他部落同一群体，但在语言结构、词汇等方面，与他们有着明显不同的区别。  因此，可以说阿美人的话语可能是在所有原住民族的語言中與他人親緣關係最遙遠。\n",
            "56 很抱歉，我無法找到相關的信息或證據來確認這句話是誰說出過。然而，這個故事可能源自於一位台灣知名程式設計老師，許志安（也稱為 \"阿宅\"）。\n",
            "57 \"embiyax namu kana \" 是阿美族的打招呼用語。\n",
            "58 根据你的问题描述，我們可以推測「鄒與布農，永久美麗」這句話可能是指位於臺灣中部的阿里山地區。   在歷史上，這裡曾經有大量 鄄族和 布农 两個民族混居，並形成了獨特文化。  因此，可以確定的是，“永遠”、“鄒與布農，永久美麗”的這句話是指位於臺灣中部的阿里山地區。\n",
            "59 很抱歉，我無法找到相關的信息或資料來回答你的問題。\n",
            "60 根据卑南族的传说，Tuku 是一位女性，她創建了 Amis 部落。\n",
            "61 《終極一班》是一部2005年播出的台湾电视剧。根据该劇的內容，KO榜（也稱為「殺破狼」）是高中生戰力排行表中的一個排名系統。  根據電視節目中的描述和情节发展，“杀”(KILL) 是 KO 排名系统第一位的人物，他被认为拥有极高战斗能力。\n",
            "62 Linux kernel 中的 Completely Fair Scheduler (CFS) 使用红黑树（Red-Black Tree）来存储排程相关信息。  在 C FS中，进 程调度器使用一个称为“runqueue”的数据结构，它是一个包含所有就绪态任务列表。这个 run queue 是用一棵自平衡二叉查找数 (Self-Balancing Binary Search Tree) 来实现的，这种树是红黑 tree。  每个进程在 red-black 树中都有一个结点，代表该过程当前状态和优先级。在这种数据结构下，每次调度时，都会找到最低时间片（time slice）剩余值的一个任务，并将其放入 CPU 执行队列。\n",
            "63 諾曼第登陸（Normandy Landings）作戰代號為「奧運會」（Operation Overlord）。這是一場盟軍在第二次世界大战中發動的最大規模海上和空降突擊行動，於1944年6月5日至9日期間進行。\n",
            "64 《Cytus II》是一款音乐节奏游戏，歌曲由多位声优演唱。根据我的信息，“Body Talk”是TOMOSUKE的角色专属主题 song\n",
            "65 根据李琳山教授的背景信息，我找到了答案。  答：演讲被称为 \"信号与系统大战\"。\n",
            "66 根据輝達的官方信息，RTX 5090 顯卡确实拥有显著提升了記憶體量。它配备有48GB GDDR6x顯存器。  这意味着，与前代产品相比，它能够提供更大的內容缓冲和处理能力，从而支持部署更加复杂的LLM模型。这也解释为什么你的朋友认为RTX 5090 顯卡在算力方面以及記憶體量上都有了显著提升。\n",
            "67 對不起，我無法提供 2024 年世界棒球12強賽的冠軍隊伍資訊，因為我沒有存取到最新或實時資料。\n",
            "68 中国四大奇书是指《西游记》、《水浒传》，以及两部小说： 《三国演义 》和  _金瓶梅_.\n",
            "69 子時是中國傳統的十二個时辰之一，對應於24小 時制中的凌晨1點到3点。\n",
            "70 在作業系統中，避免錯過時限來完成任務的排程演算法稱為「即期式」或是\"Real-time Scheduling\"(RTS)。  實際上，這類型的心臟跳動（Beat）和其他醫療監測器等應用程序需要在特定的時間內執行，否則可能會導致嚴重的後果。\n",
            "71 在《刀劍神域》中，招式代號「C8763」是由角色 \"Asuna Yuuki\" 所持有的剑技。\n",
            "72 《斯卡羅》是一部以中華民國時期為背景的電視劇，描述的是一群土匪——\"四大天王\"(包括林世榮、張文興等人)與清政府軍隊之間鬥爭。根據我的知識庫資料，這個地名「柴城」其實是指現今臺灣苗栗縣的竹南鎮。  在劇中，斯卡羅族被描繪成一群土匯組織，而這些人主要活動於山區和沿海地區。在《四大天王》系列之後，這個地名「柴城」就成為了許多觀眾心中的代表性景點。\n",
            "73 根据Google Colab的订阅制，若要使用A100高级GPU，您需要訂閱「Colaboratory Pro+」。  在 Google Collabolary 中，有三种不同的计划：  1.  免费版（Free）：提供基本功能和共享 GPU。 2\\. 「Collaboarory pro」（ ColabPro） : 提供更多的存储空间、更快速度以及优先访问GPU资源。  3.\\「Colaboratory Pro+」：这是最高级别计划，除了上述所有特性外，还提供A100高性能显卡和其他专业功能。  因此，如果您需要使用 A 1OO 高 级 GPU，则必须订阅 ColabPro + 计划才能获得此权限。\n",
            "74 根據我的知識，李宏毅老師是台湾大学資訊工程學系的教授，他開設了許多受歡迎的心臟機器学习課程。因此，我推測你要問的是：  台灣大學中由 李 宏 母 老 師 開 設 的 機 關 學 習， 是 屬 於 資 通 工 程 系 或 電 腦 科 技 研 究 所（ICT）下的「資訊工程學系」課程。  但更確切的答案是：李宏毅老師開設的心臟機器学习课程属于台湾大学電腦科學院。\n",
            "75 根據國立臺灣大學的規定，學生在每個学期都需要修滿一定數量的心力值（Credit）才可以繼續就讀。一般而言，大三学生至少需完成 24 學分。  如果雪江同伴選擇第一種策略，即多考慮一些課程來湊足不用簽減免學生申請書的機會，那麼他需要修滿一定數量的心力值（Credit）才可以避開這個程序。根據國立臺灣大學資訊工程系在 113 學年度第2学期所提供的情況，當時大三学生至少需完成24 学分。  因此，如果雪江同伴選擇第一種策略，他需要修滿不少於 **30** 心力值（Credit）才可以避開減免學生申請書的程序。\n",
            "76 Neuro-sama 是一位知名的 AI VTuber。根据我的信息， Neuro sama 的最初 Live2D 模型是使用 VTube Studio 中的一个叫做 \"VSeeFace\" 角色的模板来创建。  请注意，这个答案基于我可用的数据和知识，如果有任何更准确或最新的情况，请告诉我们，我会尽力更新。\n",
            "77 在「從零開始的異世界生活 第三季」動畫中，劫持愛蜜莉雅並想取其為妻的人是雷姆。\n",
            "78 《海綿寶宝》第五季的剧集名为\"失蹤記（Missing Identity）\", 但我找不到关于主角在哪个城市击败刺破泡沫红眼帮这一具体信息。\n",
            "79 玉米是一種雙子葉植物。它的叶片具有典型的一對小喙和一條中綴，表明其為双子的特徵之一。此外，在分類學上，也將穀物科（包括大豆、稻谷等）歸入到單亞目下的雙子葉植物門下。\n",
            "80 中華民國陸軍的军歌是《忠魂曲》，前六字为：  \"八百壯士死而後已\"  这首เพลง是在1895年甲午战争期间，由台湾日治时期的一位日本人创作，后来被改编成为中国陆海空三Military 的集体精神之歌。\n",
            "81 根據台大電資學院的規定，計算機科學與信息工程系（CSIE）是其中一個例外。這個系統允許選修一門自然科学課程，即物理、化学或生物。  因此，如果你正在考慮進入 CS IE 系列，那麼只需擇任意三门必选的课程之一即可，包括普通化学（Chemistry）、 普通 物理學 （Physics） 或 生物科目。\n",
            "82 憂傷湖（Lacus Doloris）、死lake （不確定是不是 Lacus Morti s） 、忘 lake  ( 不确定是否为 Oblivion) 和恐怖Lake(可能為Timoris ) 都位于月球的背面。\n",
            "83 《C♯小調第14號鋼琴奏鳴曲》是貝多芬的一首著名钢 piano 奏鸣，較為人知的別稱就是「月光抒情詩」（Moonlight Sonata）。這個名字源自19世紀初期一位德國音樂評論家路易斯·施波尔（Louis Spohr）所給出的暱称。\n",
            "84 阿米斯音樂節（Amnesia Music Festival）是一個國際知名的電子舞曲和嘻哈音乐节目。然而，我找不到任何信息表明它是由某位歌手所舉辦。  但我發現了一些相關資訊：   1.  阿密特·海夫納 (Amit Haviv) 是阿米斯音樂節（Amnesia Music Festival）的創始人之一。 2 .   Armin van Buuren 的 Amnezia Party 也被稱為 \"Armenian\" 或簡單地以其名字命名。\n",
            "85 \"Poppy Playtime - Chapter 4 \"是一款恐怖游戏，根据我的知识，它的黏土人角色有多个名字。其中一个最著名的是叫做\"Huggy Wug”。\n",
            "86 根據你的問題，我們可以知道賓茂部落 Djumulj 的現址是在 1951 年遷村後的新聚集地。DdjumuLJ 在排灣族語裡是「經常豐收，糧食堆積如山之處」的意思。  根據我的知識庫，我發現在行政區劃上賓茂部落 Djumulj 屬於屏東縣恆春鎮的管轄範圍。因此，即使地理位置被太麻里鄉所包夾，實際上的屬性仍然是隸属于 恒 春 镇。  所以答案就是：宾 茂 村 是 一 块 飞 地 ， 屬於屏東縣恆春鎮的管轄範圍。\n",
            "87 米開朗基羅的《大衛》雕像最初是在佛罗伦萨的一個花崗岩石坑中被創作出來。這座著名的大理壇作品是由於當時一位富有的銀行家皮耶洛·德梅迪奇（Piero de' Medici）向米開朗基羅下了一个任务，要求他在兩年內完成这件雕塑。  大約1492至1504 年間，這座《 大衛》 雛型被運到佛罗伦萨的圣洛倫佐教堂。後來因為這尊石像太過脆弱而且容易受損，於是米開朗基羅將其重建並用更坚硬、耐磨性的花崗岩取代原有的材料。  最終，這座《大衛》雕像是由佛罗伦萨的圣洛倫佐教堂向法國君主弗ラン索瓦一世贈送，於1504年被運往巴黎。\n",
            "88 根據中華民國軍事史，除了蔣介石（也就是你提到的「蒋 中正」）之外，一位曾短暫晉升特級上將的将領是張鐵聲。\n",
            "89 英雄聯盟 2012 年第二賽季世界大赛的總冠軍是韓國戰隊 SK Telecom T1。\n",
            "90 在日本麻將中，非莊家一開始的手牌通常有14張。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the results into one file.\n",
        "with open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n",
        "    for id in range(1,91):\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'r') as input_f:\n",
        "            answer = input_f.readline().strip()\n",
        "            print(answer, file=output_f)"
      ],
      "metadata": {
        "id": "ova_EcIgxmVZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}