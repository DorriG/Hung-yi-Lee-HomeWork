{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1ir32nB_4DK-2aBigiivK7I85KiY6VDj3",
      "authorship_tag": "ABX9TyPJDNzL7eLioBFl5jZbg2k/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DorriG/Hung-yi-Lee-HomeWork/blob/main/ML2025spring/HW03/LHY_HW3_Understand_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding LLM / Transformers (You cannot run the code without saving a copy)\n",
        "\n",
        "## Check the status of your GPU"
      ],
      "metadata": {
        "id": "FWOkI2yh-mt4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEsRptkm-Ygb"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing **transformers** for further usage (please do not alter the version for stable usage of model)"
      ],
      "metadata": {
        "id": "5HQLJrwe-3DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.47.0"
      ],
      "metadata": {
        "id": "lAQJ7Gje-wEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Huggingface login\n",
        "\n",
        "### You need the huggingface token (hf_token) to login to huggingface and install the gemma model. Therefore make sure you create your huggingface token. (Described in the Google slides)\n",
        "\n",
        "注意！这里的`your_hf_token`需要把权限开城public才能正常访问。\n"
      ],
      "metadata": {
        "id": "YLWRYpQ1_BQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################## TODO (Pre-requisites) ########################\n",
        "# replace `your_hf_token` with your huggingface token\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(\"your_hf_token\")\n",
        "#######################################################################"
      ],
      "metadata": {
        "id": "lwgfmiTZ-7b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the model\n",
        "\n",
        "### Gemma Model: https://huggingface.co/google/gemma-2-2b-it\n",
        "### Please accept the lincense to download the gemma model (As described on Google Slides)\n"
      ],
      "metadata": {
        "id": "hoGDTLs4_kj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_id = \"google/gemma-2-2b-it\"\n",
        "dtype = torch.float16\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=dtype,\n",
        ")"
      ],
      "metadata": {
        "id": "uq_fQQgO_FLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1: Chat template Comparison\n",
        "\n",
        "### Evaluation Model: https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2\n"
      ],
      "metadata": {
        "id": "4eZv4gzhDXGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "SCORING_MODEL = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "SCORING_TOKENIZER = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "\n",
        "def calculate_coherence(question, answer, scoring_model=SCORING_MODEL, tokenizer=SCORING_TOKENIZER):\n",
        "  features = tokenizer([question], [answer], padding=True, truncation=True, return_tensors=\"pt\")\n",
        "  scoring_model.eval()\n",
        "  with torch.no_grad():\n",
        "      scores = scoring_model(**features).logits.squeeze().item()\n",
        "  return scores"
      ],
      "metadata": {
        "id": "XxqGIhVu_qFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observe whether the chat template affects the model's output results."
      ],
      "metadata": {
        "id": "ZD40gXZcDpGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_from_prompt(prompt, tokenizer, model):\n",
        "  \"\"\"\n",
        "  generate the output from the prompt.\n",
        "  param:\n",
        "    prompt (str): the prompt inputted to the model\n",
        "    tokenizer   : the tokenizer that is used to encode / decode the input / output\n",
        "    model       : the model that is used to generate the output\n",
        "\n",
        "  return:\n",
        "    the response of the model\n",
        "  \"\"\"\n",
        "  print(\"========== Prompt inputted to the model ==========\\n\", prompt)\n",
        "\n",
        "  # Tokenize the prompt\n",
        "  input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "  ######################## TODO (Q1.1 ~ 1.4) ########################\n",
        "  ### You can refer to https://huggingface.co/google/gemma-2-2b-it for basic usage\n",
        "  ### Make sure to use 'do_sample=False' to get a deterministic response\n",
        "  ### Otherwise the coherence score may be different from the sample answer\n",
        "\n",
        "  # Generate response\n",
        "  output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=256,      # 限制生成长度\n",
        "        do_sample=False,         # 不采样，保证确定性\n",
        "        temperature=1.0,         # 温度对 do_sample=False 没影响\n",
        "        top_p=1.0                # nucleus sampling，对 deterministic 输出没影响\n",
        "    )\n",
        "  ###################################################################\n",
        "  if output_ids is not None and len(output_ids) > 0:\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "  else:\n",
        "    return \"Empty Response\""
      ],
      "metadata": {
        "id": "8y-zCMiwDc7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With chat template\n",
        "question = \"Please tell me about the key differences between supervised learning and unsupervised learning. Answer in 200 words.\"\n",
        "chat = [\n",
        "    {\"role\": \"user\", \"content\": question},\n",
        "]\n",
        "prompt_with_template = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "response_with_template = generate_text_from_prompt(prompt_with_template, tokenizer, model)\n",
        "\n",
        "# extract the real output from the model\n",
        "response_with_template = response_with_template.split('model\\n')[-1].strip('\\n').strip()\n",
        "\n",
        "print(\"========== Output ==========\\n\", response_with_template)\n",
        "score = calculate_coherence(question, response_with_template)\n",
        "print(f\"========== Coherence Score : {score:.4f}  ==========\")"
      ],
      "metadata": {
        "id": "vn_kzzM1DsUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Without chat template (directly using plain text)\n",
        "response_without_template = generate_text_from_prompt(question, tokenizer, model)\n",
        "\n",
        "# extract the real output from the model\n",
        "response_without_template = response_without_template.split(question.split(' ')[-1])[-1].strip('\\n').strip()\n",
        "print(\"========== Output ==========\\n\", response_without_template)\n",
        "score = calculate_coherence(question, response_without_template)\n",
        "print(f\"========== Coherence Score : {score:.4f}  ==========\")"
      ],
      "metadata": {
        "id": "7iL8yB5wES6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2: Multi-turn conversations"
      ],
      "metadata": {
        "id": "2vAqGbUVEgdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "chat_history = []\n",
        "round = 0\n",
        "print(\"Chatbot: Hello! How can I assist you today? (Type 'exit' to quit)\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    round += 1\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "    chat_template_format_prompt = tokenizer.apply_chat_template(chat_history, tokenize=False, add_generation_prompt=True)\n",
        "    ######################## (Q2.1 ~ 2.3) ########################\n",
        "    # Observe the prompt with chat template format that was inputted to the model in the current round to answer Q2.1 ~ Q2.3.\n",
        "    print(f\"=== Prompt with chat template format inputted to the model on round {round} ===\\n{chat_template_format_prompt}\")\n",
        "    print(f\"===============================================\")\n",
        "    ###################################################################\n",
        "\n",
        "    inputs = tokenizer(chat_template_format_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    # Get logits instead of directly generating\n",
        "    with torch.no_grad():\n",
        "        outputs_p = model(**inputs)\n",
        "\n",
        "    logits = outputs_p.logits  # Logits of the model (raw scores before softmax)\n",
        "    last_token_logits = logits[:, -1, :]  # Take the logits of the last generated token\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probs = torch.nn.functional.softmax(last_token_logits, dim=-1)\n",
        "\n",
        "    # Get top-k tokens (e.g., 10)\n",
        "    top_k = 10\n",
        "    top_probs, top_indices = torch.topk(probs, top_k)\n",
        "\n",
        "    # Convert to numpy for plotting\n",
        "    top_probs = top_probs.cpu().squeeze().numpy()\n",
        "    top_indices = top_indices.cpu().squeeze().numpy()\n",
        "    top_tokens = [tokenizer.decode([idx]) for idx in top_indices]\n",
        "\n",
        "    # Plot probability distribution\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.barplot(x=top_probs, y=top_tokens, palette=\"coolwarm\")\n",
        "    plt.xlabel(\"Probability\")\n",
        "    plt.ylabel(\"Token\")\n",
        "    plt.title(\"Top Token Probabilities for Next Word\")\n",
        "    plt.show()\n",
        "\n",
        "    # Generate response\n",
        "    outputs = model.generate(**inputs, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
        "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"Chatbot: {response}\")\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": response})"
      ],
      "metadata": {
        "id": "C_dX4efjEc2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3: Tokenization of Sentence"
      ],
      "metadata": {
        "id": "ZBcyli1RE5jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I love taking a Machine Learning course by Professor Hung-yi Lee, What about you?\" #@param {type:\"string\"}\n",
        "\n",
        "######################## TODO (Q3.1 ~ 3.4) ########################\n",
        "### You can refer to https://huggingface.co/learn/nlp-course/en/chapter2/4?fw=pt for basic tokenizer usage\n",
        "### and https://huggingface.co/docs/transformers/en/main_classes/tokenizer for full tokenizer usage\n",
        "\n",
        "\n",
        "# Encode the sentence into token IDs without adding special tokens\n",
        "token_ids = tokenizer.encode(sentence, add_special_tokens=False)\n",
        "\n",
        "# Convert the token IDs back to their corresponding tokens (words or subwords)\n",
        "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "###################################################################\n",
        "\n",
        "# Iterate through the tokens and their corresponding token IDs\n",
        "for t, t_id in zip(tokens, token_ids):\n",
        "    # Print the token and its index (ID)\n",
        "    print(f\"Token: {t}, token index: {t_id}\")"
      ],
      "metadata": {
        "id": "STrlvyqCEjLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4: Auto-regressive generation\n"
      ],
      "metadata": {
        "id": "SVEWVojqFXBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import trange\n",
        "from transformers import HybridCache\n",
        "\n",
        "max_generation_tokens = 30\n",
        "\n",
        "######################## TODO (Q4.3 ~ 4.6) ########################\n",
        "# Modify the value of k and p accordingly\n",
        "\n",
        "top_k = 2  # Set K for top-k sampling\n",
        "top_p = 0.6  # Set P for nucleus sampling\n",
        "###################################################################\n",
        "\n",
        "# Input prompt\n",
        "prompt = f\"Generate a paraphrase of the sentence 'Professor Hung-yi Lee is one of the best teachers in the domain of machine learning'. Just response with one sentence.\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Initialize KV Cache\n",
        "kv_cache = HybridCache(config=model.config, max_batch_size=1, max_cache_len=max_generation_tokens, device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "next_token_id = input_ids.input_ids.to(\"cuda\")\n",
        "attention_mask = input_ids.attention_mask.to(\"cuda\")\n",
        "cache_position = torch.arange(attention_mask.shape[1], device=\"cuda\")\n",
        "\n",
        "generated_sentences_top_k = []\n",
        "generated_sentences_top_p = []\n",
        "\n",
        "\n",
        "\n",
        "# Define the generation parameters\n",
        "generation_params = {\n",
        "    \"do_sample\": True,  # Enable sampling\n",
        "    \"max_length\": max_generation_tokens + len(input_ids.input_ids[0]),  # Total length including prompt\n",
        "    \"pad_token_id\": tokenizer.pad_token_id,  # Ensure padding token is set\n",
        "    \"eos_token_id\": tokenizer.eos_token_id,  # Ensure EOS token is set\n",
        "    \"bos_token_id\": tokenizer.bos_token_id,  # Ensure BOS token is set\n",
        "    \"attention_mask\": input_ids.attention_mask.to(\"cuda\"),  # Move attention mask to GPU\n",
        "    \"use_cache\": True,  # Enable caching\n",
        "    \"return_dict_in_generate\": True,  # Return generation outputs\n",
        "    \"output_scores\": False,  # Disable outputting scores\n",
        "}\n",
        "\n",
        "\n",
        "for method in [\"top-k\", \"top-p\"]:\n",
        "    for _ in trange(20):\n",
        "      if method == \"top-k\":\n",
        "        # Generate text using the model with top_k\n",
        "        generated_output = model.generate(\n",
        "            input_ids=input_ids.input_ids.to(\"cuda\"),\n",
        "            top_k=top_k,\n",
        "            **generation_params\n",
        "        )\n",
        "      elif method == \"top-p\":\n",
        "        # Generate text using the model with top_p\n",
        "        ######################## TODO (Q4.3 ~ 4.6) ########################\n",
        "        # Generate output from the model based on the input_ids and specified generation parameters\n",
        "        # You can refer to this documentation: https://huggingface.co/docs/transformers/en/main_classes/text_generation\n",
        "        # Hint: You can check how we generate the text with top_k\n",
        "\n",
        "        generated_output = model.generate(\n",
        "            input_ids=input_ids.input_ids.to(\"cuda\"),\n",
        "            top_p=top_p,\n",
        "            **generation_params\n",
        "        )\n",
        "        ###################################################################\n",
        "      else:\n",
        "        raise NotImplementedError()\n",
        "      # Decode the generated tokens\n",
        "      generated_tokens = generated_output.sequences[0, len(input_ids.input_ids[0]):]\n",
        "      decoded_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "      # Combine the prompt with the generated text\n",
        "      sentence = decoded_text.replace(\" ,\", \",\").replace(\" 's\", \"'s\").replace(\" .\", \".\").strip()\n",
        "\n",
        "      # Append the generated sentence to the appropriate list\n",
        "      if method == \"top-k\":\n",
        "          generated_sentences_top_k.append(sentence)\n",
        "      else:\n",
        "          generated_sentences_top_p.append(sentence)\n",
        "\n",
        "# Print results\n",
        "print(\"===== Top-K Sampling Output =====\")\n",
        "print()\n",
        "for idx,sentence in enumerate(generated_sentences_top_k):\n",
        "    print(f\"{idx}. {sentence}\")\n",
        "print()\n",
        "print(\"===== Top-P Sampling Output =====\")\n",
        "print()\n",
        "for idx,sentence in enumerate(generated_sentences_top_p):\n",
        "    print(f\"{idx}. {sentence}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "AxmqJ0EpE8xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def compute_self_bleu(generated_sentences):\n",
        "    total_bleu_score = 0\n",
        "    num_sentences = len(generated_sentences)\n",
        "\n",
        "    for i, hypothesis in enumerate(generated_sentences):\n",
        "        references = [generated_sentences[j] for j in range(num_sentences) if j != i]\n",
        "        bleu_scores = [sentence_bleu([ref.split()], hypothesis.split()) for ref in references]\n",
        "        total_bleu_score += sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "    return total_bleu_score / num_sentences\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = compute_self_bleu(generated_sentences_top_k)\n",
        "print(f\"self-BLEU Score for top_k (k={top_k}): {bleu_score:.4f}\")\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = compute_self_bleu(generated_sentences_top_p)\n",
        "print(f\"self-BLEU Score for top_p (p={top_p}): {bleu_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "GxRrDTwrFaFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5: t-SNE"
      ],
      "metadata": {
        "id": "IODzWxa0GA0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "######################## (Q5.2 ~ 5.3) ########################\n",
        "# Sentences with different meanings of words\n",
        "sentences = [\n",
        "    \"I ate a fresh apple.\",  # Apple (fruit)\n",
        "    \"Apple released the new iPhone.\",  # Apple (company)\n",
        "    \"I peeled an orange and ate it.\",  # Orange (fruit)\n",
        "    \"The Orange network has great coverage.\",  # Orange (telecom)\n",
        "    \"Microsoft announced a new update.\",  # Microsoft (company)\n",
        "    \"Banana is my favorite fruit.\",  # Banana (fruit)\n",
        "]\n",
        "\n",
        "# Tokenize and move to device\n",
        "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "inputs = inputs.to(device)\n",
        "\n",
        "# Get hidden states\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "hidden_states = outputs.hidden_states[-1]  # Extract last layer embeddings\n",
        "\n",
        "# Compute sentence-level embeddings (mean pooling)\n",
        "sentence_embeddings = hidden_states.mean(dim=1).cpu().numpy()\n",
        "\n",
        "# Words to visualize\n",
        "word_labels = [\n",
        "    \"Apple (fruit)\", \"Apple (company)\",\n",
        "    \"Orange (fruit)\", \"Orange (telecom)\",\n",
        "    \"Microsoft (company)\", \"Banana (fruit)\"\n",
        "]\n",
        "\n",
        "# Reduce to 2D using t-SNE\n",
        "tsne = TSNE(n_components=2, perplexity=2, random_state=42)\n",
        "embeddings_2d = tsne.fit_transform(sentence_embeddings)\n",
        "\n",
        "# Plot the embeddings\n",
        "plt.figure(figsize=(8, 6))\n",
        "colors = [\"red\", \"blue\", \"orange\", \"purple\", \"green\", \"brown\"]\n",
        "for i, label in enumerate(word_labels):\n",
        "    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], color=colors[i], s=100)\n",
        "    plt.text(embeddings_2d[i, 0] + 0.1, embeddings_2d[i, 1] + 0.1, label, fontsize=12, color=colors[i])\n",
        "\n",
        "plt.xlabel(\"t-SNE Dim 1\")\n",
        "plt.ylabel(\"t-SNE Dim 2\")\n",
        "plt.title(\"t-SNE Visualization of Word Embeddings\")\n",
        "plt.show()\n",
        "##################################################"
      ],
      "metadata": {
        "id": "DpS0rWduF_X0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6: Observe the Attention Weight"
      ],
      "metadata": {
        "id": "Qs64ZPJeGIjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm import trange\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Input prompt for text generation\n",
        "prompt = \"Google \"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\")  # Tokenize the input prompt\n",
        "next_token_id = input_ids.input_ids.to(\"cuda\")  # Move input token ids to GPU\n",
        "attention_mask = input_ids.attention_mask.to(\"cuda\")  # Move attention mask to GPU\n",
        "cache_position = torch.arange(attention_mask.shape[1], device=\"cuda\")  # Position for the KV cache\n",
        "\n",
        "# Set the number of tokens to generate and other parameters\n",
        "generation_tokens = 20  # Limit for visualization (number of tokens to generate)\n",
        "total_tokens = generation_tokens + next_token_id.size(1) - 1  # Total tokens to handle\n",
        "layer_idx = 10  # Specify the layer index for attention visualization\n",
        "head_idx = 7  # Specify the attention head index to visualize\n",
        "\n",
        "# KV cache setup for caching key/values across time steps\n",
        "from transformers.cache_utils import HybridCache\n",
        "kv_cache = HybridCache(config=model.config, max_batch_size=1, max_cache_len=total_tokens, device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "generated_tokens = []  # List to store generated tokens\n",
        "attentions = None  # Placeholder to store attention weights\n",
        "\n",
        "num_new_tokens = 0  # Counter for the number of new tokens generated\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "\n",
        "# Generate tokens and collect attention weights for visualization\n",
        "for num_new_tokens in range(generation_tokens):\n",
        "    with torch.no_grad():  # Disable gradients during inference for efficiency\n",
        "        # Pass the input through the model to get the next token prediction and attention weights\n",
        "        outputs = model(\n",
        "            next_token_id,\n",
        "            attention_mask=attention_mask,\n",
        "            cache_position=cache_position,\n",
        "            use_cache=True,  # Use the KV cache for efficiency\n",
        "            past_key_values=kv_cache,  # Provide the cached key-value pairs for fast inference\n",
        "            output_attentions=True  # Enable the extraction of attention weights\n",
        "        )\n",
        "\n",
        "    ######################## TODO (Q6.1 ~ 6.4) ########################\n",
        "    ### You can refer to https://huggingface.co/docs/transformers/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput.attentions to see the structure of model output attentions\n",
        "    # Get the logits for the last generated token from outputs\n",
        "    logits = outputs.logits[:, -1, :]\n",
        "\n",
        "    # Extract the attention scores from the model's outputs\n",
        "    attention_scores = outputs.attentions\n",
        "    ###################################################################\n",
        "\n",
        "    # Extract attention weights for the specified layer and head\n",
        "    last_layer_attention = attention_scores[layer_idx][0][head_idx].detach().cpu().numpy()\n",
        "\n",
        "    # If it's the first generated token, initialize the attentions array\n",
        "    if num_new_tokens == 0:\n",
        "        attentions = last_layer_attention\n",
        "    else:\n",
        "        # Append the current attention weights to the existing array\n",
        "        attentions = np.append(attentions, last_layer_attention, axis=0)\n",
        "\n",
        "    # Choose the next token to generate based on the highest probability (logits)\n",
        "    next_token_id = logits.argmax(dim=-1)\n",
        "    generated_tokens.append(next_token_id.item())  # Add the token ID to the generated tokens list\n",
        "\n",
        "    # Update the attention mask and next token ID for the next iteration\n",
        "    attention_mask = torch.cat([attention_mask, torch.ones(1, 1, device=\"cuda\")], dim=-1)  # Add a new attention mask for the generated token\n",
        "    next_token_id = next_token_id.unsqueeze(0)  # Convert the token ID to the required shape\n",
        "\n",
        "    # Update the KV cache with the new past key-values\n",
        "    kv_cache = outputs.past_key_values\n",
        "    cache_position = cache_position[-1:] + 1  # Update the cache position for the next iteration\n",
        "\n",
        "# Decode the generated tokens into human-readable text\n",
        "generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "full_text = prompt + generated_text  # Combine the prompt with the generated text\n",
        "\n",
        "# Tokenize all the generated text (prompt + generated)\n",
        "tokens = tokenizer.tokenize(full_text)\n",
        "\n",
        "# Function to plot a heatmap of attention weights\n",
        "def plot_attention(attn_matrix, tokens, title=\"Attention Heatmap\"):\n",
        "    plt.figure(figsize=(10, 8))  # Set the figure size\n",
        "    sns.heatmap(attn_matrix, xticklabels=tokens, yticklabels=tokens, cmap=\"viridis\", annot=False)  # Plot the attention matrix as a heatmap\n",
        "    plt.xlabel(\"Key Tokens\")\n",
        "    plt.ylabel(\"Query Tokens\")\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
        "    plt.yticks(rotation=0)  # Rotate y-axis labels\n",
        "    plt.show()\n",
        "\n",
        "# Plot the attention heatmap for the last generated token\n",
        "plot_attention(attentions, tokens, title=f\"Attention Weights for Generated Token of Layer {layer_idx}\")"
      ],
      "metadata": {
        "id": "YphGN7xaGFVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7: Observe the Activation Scores\n",
        "\n",
        "The following code is referred from official Gemma tutorials: [Gemma Tutorial From Scratch](https://colab.research.google.com/drive/17dQFYUYnuKnP6OwQPH9v_GSYUW5aj-Rp#scrollTo=2-i7YRVLgKoT) and [SAELens](https://github.com/jbloomAus/SAELens/blob/main/tutorials/tutorial_2_0.ipynb)"
      ],
      "metadata": {
        "id": "OFMaW_cfGiWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sae-lens"
      ],
      "metadata": {
        "id": "mtp7PrXHGK85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sae_lens import SAE\n",
        "\n",
        "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
        "    release = \"gemma-scope-2b-pt-res-canonical\",\n",
        "    sae_id = \"layer_20/width_16k/canonical\",\n",
        ")\n",
        "\n",
        "print(sae, cfg_dict, sparsity)\n"
      ],
      "metadata": {
        "id": "ZzBo1OTGGoBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7.2~7.3: Maximum activations comparison"
      ],
      "metadata": {
        "id": "BUXNLEGJIFgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_activation(model, tokenizer, sae, prompt, feature_idx=10004, layer_idx=-1):\n",
        "    \"\"\"\n",
        "    Computes the maximum activation of a specific feature in a Sparse Autoencoder (SAE)\n",
        "    for a given prompt, and plots the distribution.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    sae.to(device)\n",
        "\n",
        "    # Tokenize input and run model\n",
        "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model(tokens, output_hidden_states=True)\n",
        "\n",
        "    # Pick the hidden states from the chosen layer\n",
        "    hidden_states = outputs.hidden_states[layer_idx].to(device)  # default: last layer\n",
        "\n",
        "    # Flatten hidden states: (batch*seq_len, hidden_dim)\n",
        "    sae_in = hidden_states.reshape(-1, hidden_states.shape[-1])\n",
        "\n",
        "    # SAE encode -> (batch*seq_len, num_features)\n",
        "    feature_acts = sae.encode(sae_in)\n",
        "\n",
        "    # Compute max activation for the specific feature\n",
        "    max_activation = feature_acts[:, feature_idx].max().item()\n",
        "\n",
        "    # Plot histogram of feature activations\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.hist(\n",
        "        feature_acts[:, feature_idx].detach().cpu().numpy(),\n",
        "        bins=50,\n",
        "        alpha=0.75,\n",
        "        color='blue',\n",
        "        edgecolor='black'\n",
        "    )\n",
        "    plt.xlabel(f\"Activation values (Feature {feature_idx})\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(f\"Activation Distribution for Feature {feature_idx}\\nPrompt: '{prompt}'\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return max_activation\n",
        "\n",
        "\n",
        "feature_idx = 10004\n",
        "prompt_a = \"Time travel offers me the opportunity to correct past errors, but it comes with its own set of risks.\"\n",
        "prompt_b = \"I accept that my decisions shape my future, and though mistakes are inevitable, they define who I become.\"\n",
        "\n",
        "max_activation_a = get_max_activation(model, tokenizer, sae, prompt_a, feature_idx)\n",
        "max_activation_b = get_max_activation(model, tokenizer, sae, prompt_b, feature_idx)\n",
        "\n",
        "print(f\"max_activation for prompt_a: {max_activation_a}\")\n",
        "print(f\"max_activation for prompt_b: {max_activation_b}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "uQlg6vefIGFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7.4~7.6: Activation distribution for specific layer"
      ],
      "metadata": {
        "id": "EJRGay0BP6w6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_token_activations(model, tokenizer, sae, prompt, feature_idx=10004, layer_idx=0):\n",
        "    \"\"\"\n",
        "    Plots activations for each token in a specific layer.\n",
        "\n",
        "    Args:\n",
        "        model: The transformer model.\n",
        "        tokenizer: Tokenizer for encoding input text.\n",
        "        sae: Sparse Autoencoder model.\n",
        "        prompt: Input text string.\n",
        "        feature_idx: Index of the feature to analyze.\n",
        "        layer_idx: Layer to analyze (None uses sae.cfg.hook_layer).\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    sae.to(device)\n",
        "\n",
        "    # Tokenize input and get model output\n",
        "    tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    token_ids = tokens[\"input_ids\"].to(device)\n",
        "    token_list = tokenizer.convert_ids_to_tokens(token_ids.squeeze().tolist())\n",
        "\n",
        "    outputs = model(token_ids, output_hidden_states=True)\n",
        "\n",
        "    # Choose layer\n",
        "    layer_idx = layer_idx if layer_idx is not None else sae.cfg.hook_layer\n",
        "    hidden_states = outputs.hidden_states[layer_idx]\n",
        "\n",
        "    # Pass through SAE\n",
        "    sae_in = hidden_states\n",
        "    feature_acts = sae.encode(sae_in).squeeze()  # (batch_size, seq_len, num_features)\n",
        "    print(f\"feature_acts shape: {feature_acts.shape}\")\n",
        "\n",
        "    # Extract activations for the chosen feature\n",
        "    activations = feature_acts[:, feature_idx].squeeze().cpu().detach().numpy()\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(range(len(token_list)), activations, color='blue', alpha=0.7)\n",
        "    plt.xticks(range(len(token_list)), token_list, rotation=45)\n",
        "    plt.xlabel(\"Tokens\")\n",
        "    plt.ylabel(f\"Activation Value (Feature {feature_idx})\")\n",
        "    plt.title(f\"Token-wise Activations for Layer {layer_idx}\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "######################## (Q7.4 ~ 7.6) ########################\n",
        "# Simply observe the figure\n",
        "layer_idx = 24\n",
        "prompt = \"Time travel will become a reality as technology continues to advance.\"\n",
        "plot_token_activations(model, tokenizer, sae, prompt, feature_idx, layer_idx)\n",
        "###################################################################"
      ],
      "metadata": {
        "id": "unUEfErSII7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7.7~7.9: Activation distribution for specific token"
      ],
      "metadata": {
        "id": "zkQGUBOJQAes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_layer_activations(model, tokenizer, sae, prompt, token_idx=0, feature_idx=10004):\n",
        "    \"\"\"\n",
        "    Plots activations of a specific token across all layers.\n",
        "\n",
        "    Args:\n",
        "        model: The transformer model.\n",
        "        tokenizer: Tokenizer for encoding input text.\n",
        "        sae: Sparse Autoencoder model.\n",
        "        prompt: Input text string.\n",
        "        token_idx: Index of the token to analyze.\n",
        "        feature_idx: Index of the feature to analyze.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    sae.to(device)\n",
        "\n",
        "    # Tokenize input and get model output\n",
        "    tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    token_ids = tokens[\"input_ids\"].to(device)\n",
        "    token_list = tokenizer.convert_ids_to_tokens(token_ids.squeeze().tolist())\n",
        "\n",
        "    outputs = model(token_ids, output_hidden_states=True)\n",
        "\n",
        "    # Collect activations across all layers\n",
        "    num_layers = len(outputs.hidden_states)\n",
        "    activations = []\n",
        "\n",
        "    for layer_idx in range(num_layers):\n",
        "        hidden_states = outputs.hidden_states[layer_idx]\n",
        "        sae_in = hidden_states\n",
        "        feature_acts = sae.encode(sae_in).squeeze()  # (batch_size, seq_len, num_features)\n",
        "        # print(f\"feature_acts shape: {feature_acts.shape}\")\n",
        "        activations.append(feature_acts[token_idx, feature_idx].item())\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(range(num_layers), activations, marker=\"o\", linestyle=\"-\", color=\"blue\")\n",
        "    plt.xlabel(\"Layer\")\n",
        "    plt.ylabel(f\"Activation Value (Feature {feature_idx})\")\n",
        "    plt.title(f\"Activation Across Layers for Token '{token_list[token_idx]}'\")\n",
        "    plt.xticks(range(num_layers))\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "######################## (Q7.7 ~ 7.9) ########################\n",
        "# Alter the token index to observe the figure\n",
        "token_idx = 1\n",
        "prompt = \"Time travel will become a reality as technology continues to advance.\"\n",
        "plot_layer_activations(model, tokenizer, sae, prompt, token_idx)\n",
        "###################################################################"
      ],
      "metadata": {
        "id": "tVCJ3-n0P-fJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}