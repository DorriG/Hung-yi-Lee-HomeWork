{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "xJcZwF4NM5xx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luoclab/Hung-yi-Lee-HomeWork/blob/main/GenerativeAI2024spring/HW10/GenAI_HW10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GenAI HW10: Stable Diffusion Fine-tuning\n",
        "In this homework, you will fine-tune your own Stable Diffusion model to generate your customized images from given text description. For more details, please refer to homework slides\n",
        "\n",
        "## **TODOs**\n",
        "\n",
        "1. Read the slides and make sure you know the objectives of this homework.\n",
        "2. Save a copy of this Colab notebook.\n",
        "3. Follow the steps in this Colab notebook to fine-tune your Stable Diffusion.\n",
        "4. Evaluate outputs using FaceNet and CLIP\n",
        "5. Update results and datasets to NTU COOL\n",
        "If you have any questions, please contact the TAs via TA hours, NTU COOL, or email to ntu-gen-ai-2024-spring-ta@googlegroups.com\n",
        "\n",
        "Tips: At the top of each cell, it shows whether you need to change hyperparameters in that cell and how long it might take.\n",
        "\n",
        "This is based on the work of [Hugging Face](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py).\n",
        "\n",
        "And special thanks to [Celebrity Face Image Dataset](https://www.kaggle.com/datasets/vishesh1412/celebrity-face-image-dataset).\n",
        "\n",
        "\n",
        "Thank you!\n"
      ],
      "metadata": {
        "id": "CnJtiRaRuTFX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNjz10u8biUn",
        "outputId": "586f6a9c-a6ff-49b2-c082-13e08a58ef76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ Connecting to Google Drive...\n",
            "Mounted at /content/drive\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1OXPG2vNb8bG2334HML8vKpqo8UbAEV3d\n",
            "To: /content/drive/MyDrive/GenAI-HW10/Datasets.zip\n",
            "100% 2.78M/2.78M [00:00<00:00, 180MB/s]\n",
            "fatal: destination path '/content/drive/MyDrive/GenAI-HW10/Brad/logs' already exists and is not an empty directory.\n",
            "âœ… Project Brad is ready!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "COLAB = True\n",
        "\n",
        "if COLAB:\n",
        "    from google.colab.output import clear as clear_output\n",
        "else:\n",
        "    from IPython.display import clear_output\n",
        "\n",
        "\n",
        "#@markdown ## Link to Google Drive\n",
        "#@markdown This cell will load some requirements and create the necessary folders in your Google Drive. <p>\n",
        "#@markdown Your project name can't contain spaces but it can contain a single / to make a subfolder in your dataset.\n",
        "project_name = \"Brad\" #@param {type:\"string\"}\n",
        "project_name = project_name.strip()\n",
        "# dataset_name = \"Brad-512\" #@param [\"Brad-512\", \"Anne-512\"]\n",
        "dataset_name = \"Brad\"\n",
        "\n",
        "if not project_name or any(c in project_name for c in \" .()\\\"'\\\\\") or project_name.count(\"/\") > 1:\n",
        "    print(\"Please write a valid project_name.\")\n",
        "else:\n",
        "    if COLAB and not os.path.exists('/content/drive'):\n",
        "      from google.colab import drive\n",
        "      print(\"ğŸ“‚ Connecting to Google Drive...\")\n",
        "      drive.mount('/content/drive')\n",
        "\n",
        "    project_base = project_name if \"/\" not in project_name else project_name[:project_name.rfind(\"/\")]\n",
        "    project_subfolder = project_name if \"/\" not in project_name else project_name[project_name.rfind(\"/\")+1:]\n",
        "\n",
        "    root_dir = \"/content\" if COLAB else \"~/Loras\"\n",
        "    main_dir        = os.path.join(root_dir, \"drive/MyDrive/GenAI-HW10\") if COLAB else root_dir\n",
        "    project_dir =  os.path.join(main_dir, project_name)\n",
        "    os.makedirs(main_dir, exist_ok=True)\n",
        "    os.makedirs(project_dir, exist_ok=True)\n",
        "    zip_file = os.path.join(main_dir, \"Datasets.zip\")\n",
        "    !gdown 1OXPG2vNb8bG2334HML8vKpqo8UbAEV3d -O {zip_file}\n",
        "    !unzip -q -o {zip_file} -d {main_dir}\n",
        "    log_file = os.path.join(project_dir, \"logs.zip\")\n",
        "    log_dir = os.path.join(project_dir, \"logs\")\n",
        "    !git clone https://huggingface.co/yahcreeper/GenAI-HW10-Model {log_dir}\n",
        "    # !cd {log_dir}\n",
        "    # !git lfs pull\n",
        "    # !gdown 1kalT3k7kEV0xcD6pf_OTSo7npHmsfZ_z -O {log_file}\n",
        "    # !unzip -q -o {log_file} -d {project_dir}\n",
        "    # !rm -f {log_file}\n",
        "    model_path = os.path.join(project_dir, \"logs\", \"checkpoint-last\")\n",
        "    images_folder   = os.path.join(main_dir, \"Datasets\", dataset_name)\n",
        "    prompts_folder  = os.path.join(main_dir, \"Datasets\", \"prompts\")\n",
        "    captions_folder = images_folder\n",
        "    os.makedirs(images_folder, exist_ok=True)\n",
        "\n",
        "    print(f\"âœ… Project {project_name} is ready!\")\n",
        "    step1_installed_flag = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##  Install the required packages\n",
        "#@markdown In this session, we will install some well-established packages to facilitate the fine-tuning process. <p>\n",
        "#@markdown The installation will take about 5 minutes.\n",
        "os.chdir(root_dir)\n",
        "!pip -q install torch==2.5 timm==1.0.7 fairscale==0.4.13 transformers\n",
        "!pip -q install requests accelerate==0.31.0 diffusers==0.29.1 einop==0.0.1 safetensors==0.4.3 voluptuous==0.15.1\n",
        "!pip -q install jax peft==0.11.1 deepface==0.0.92 tensorflow keras"
      ],
      "metadata": {
        "id": "lJ4pB6-_dC91",
        "outputId": "4a7fcda8-a1e4-48bb-def4-eb52523033c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/47.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/266.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m906.5/906.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for voluptuous (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "ERROR: unknown command \"jax\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TVrA9vslTegY",
        "outputId": "3a24b9b7-c382-4a2f-867d-a83abc03537a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/87.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.5/105.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.1/86.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.6 which is incompatible.\n",
            "orbax-checkpoint 0.11.10 requires jax>=0.5.0, but you have jax 0.4.34 which is incompatible.\n",
            "tensorstore 0.1.72 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.15.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##  Import necessary packages\n",
        "#@markdown It is recommmended NOT to change codes in this cell.\n",
        "import argparse\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import glob\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "import transformers\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "from tqdm.auto import tqdm\n",
        "from peft import LoraConfig\n",
        "from peft.utils import get_peft_model_state_dict\n",
        "from transformers import AutoProcessor, AutoModel, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "import diffusers\n",
        "from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, StableDiffusionPipeline, UNet2DConditionModel\n",
        "from diffusers.optimization import get_scheduler\n",
        "from diffusers.utils import convert_state_dict_to_diffusers\n",
        "from diffusers.training_utils import compute_snr\n",
        "from diffusers.utils.torch_utils import is_compiled_module\n",
        "from deepface import DeepFace\n",
        "import cv2"
      ],
      "metadata": {
        "id": "pusB39oYsycq",
        "outputId": "c9a4035d-c1c6-4b79-f63c-030f98bc0d39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your environment has TF_USE_LEGACY_KERAS set to True, but you do not have the tf_keras package installed. You must install it in order to use the legacy tf.keras. Install it via: `pip install tf_keras`\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Keras cannot be imported. Check that it is installed.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ea4969cccd98>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_snr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_compiled_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeepFace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepface/DeepFace.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpackage_utils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m from deepface.modules import (\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mmodeling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mrepresentation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepface/modules/modeling.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# project dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m from deepface.basemodels import (\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mVGGFace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mOpenFace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepface/basemodels/VGGFace.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpackage_utils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mverification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFacialRecognition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFacialRecognition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepface/modules/verification.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# project dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrepresentation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFacialRecognition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFacialRecognition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepface/modules/representation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# project dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFacialRecognition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFacialRecognition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepface/modules/detection.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDetector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetectedFace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFacialAreaRegion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetectorWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepface/detectors/DetectorWrapper.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDetector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDetectedFace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFacialAreaRegion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from deepface.detectors import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mFastMtCnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mMediaPipe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepface/detectors/MtCnn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmtcnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDetector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFacialAreaRegion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mtcnn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# SOFTWARE.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmtcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmtcnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"MTCNN\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mtcnn/mtcnn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmtcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStagePNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStageRNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStageONet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmtcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_images_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandarize_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mtcnn/stages/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# SOFTWARE.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmtcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage_pnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStagePNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmtcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage_rnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStageRNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmtcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage_onet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStageONet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mtcnn/stages/stage_pnet.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmtcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmtcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mtcnn/network/pnet.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    179\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m       return (f\"<KerasLazyLoader ({self._keras_version}) \"\n",
            "\u001b[0;31mImportError\u001b[0m: Keras cannot be imported. Check that it is installed.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change the following parameters, or the process may crashed due to GPU out of memory.\n",
        "output_folder = os.path.join(project_dir, \"logs\") # å­˜æ”¾model checkpointsè·Ÿvalidationçµæœçš„è³‡æ–™å¤¾\n",
        "seed = 1126 # random seed\n",
        "train_batch_size = 2 # training batch size\n",
        "resolution = 512 # Image size\n",
        "weight_dtype = torch.bfloat16 #\n",
        "snr_gamma = 5\n",
        "#####\n",
        "\n",
        "#@markdown ## Important parameters for fine-tuning Stable Diffusion\n",
        "pretrained_model_name_or_path = \"stablediffusionapi/cyberrealistic-41\"\n",
        "lora_rank = 32\n",
        "lora_alpha = 16\n",
        "#@markdown ### â–¶ï¸ Learning Rate\n",
        "#@markdown The learning rate is the most important for your results. If you want to train slower with lots of images, or if your dim and alpha are high, move the unet to 2e-4 or lower. <p>\n",
        "#@markdown The text encoder helps your Lora learn concepts slightly better. It is recommended to make it half or a fifth of the unet. If you're training a style you can even set it to 0.\n",
        "learning_rate = 1e-4 #@param {type:\"number\"}\n",
        "unet_learning_rate = learning_rate\n",
        "text_encoder_learning_rate = learning_rate\n",
        "lr_scheduler_name = \"cosine_with_restarts\" # è¨­å®šå­¸ç¿’ç‡çš„æ’ç¨‹\n",
        "lr_warmup_steps = 100 # è¨­å®šç·©æ…¢æ›´æ–°çš„æ­¥æ•¸\n",
        "#@markdown ### â–¶ï¸ Steps\n",
        "#@markdown Choose your training step and the number of generated images per each validaion\n",
        "max_train_steps = 200 #@param {type:\"slider\", min:200, max:2000, step:100}\n",
        "validation_prompt = \"validation_prompt.txt\"\n",
        "validation_prompt_path = os.path.join(prompts_folder, validation_prompt)\n",
        "validation_prompt_num = 3 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "validation_step_ratio = 1 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "with open(validation_prompt_path, \"r\") as f:\n",
        "    validation_prompt = [line.strip() for line in f.readlines()]\n",
        "#####"
      ],
      "metadata": {
        "id": "oTbop_tCsxIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Some Useful Functions and Class"
      ],
      "metadata": {
        "id": "bpPjMo_IMtt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "IMAGE_EXTENSIONS = [\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".PNG\", \".JPG\", \".JPEG\", \".WEBP\", \".BMP\"]\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "        transforms.CenterCrop(resolution),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "    ]\n",
        ")\n",
        "class Text2ImageDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    (1) Goal:\n",
        "        - This class is used to build dataset for finetuning text-to-image model\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, images_folder, captions_folder, transform, tokenizer):\n",
        "        \"\"\"\n",
        "        (2) Arguments:\n",
        "            - images_folder: str, path to images\n",
        "            - captions_folder: str, path to captions\n",
        "            - transform: function, turn raw image into torch.tensor\n",
        "            - tokenizer: CLIPTokenize, turn sentences into word ids\n",
        "        \"\"\"\n",
        "        self.image_paths = []\n",
        "        for ext in IMAGE_EXTENSIONS:\n",
        "            self.image_paths.extend(glob.glob(f\"{images_folder}/*{ext}\"))\n",
        "        self.image_paths = sorted(self.image_paths)\n",
        "        self.train_emb = torch.tensor([DeepFace.represent(img_path, detector_backend=\"ssd\", model_name=\"GhostFaceNet\", enforce_detection=False)[0]['embedding'] for img_path in self.image_paths])\n",
        "        caption_paths = sorted(glob.glob(f\"{captions_folder}/*txt\"))\n",
        "        captions = []\n",
        "        for p in caption_paths:\n",
        "            with open(p, \"r\") as f:\n",
        "                captions.append(f.readline())\n",
        "        inputs = tokenizer(\n",
        "            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        self.input_ids = inputs.input_ids\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        input_id = self.input_ids[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            # convert to tensor temporarily so dataloader will accept it\n",
        "            tensor = self.transform(image)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load image path: {img_path}, error: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "        return tensor, input_id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "\n",
        "def prepare_lora_model(pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\", model_path=None):\n",
        "    \"\"\"\n",
        "    (1) Goal:\n",
        "        - This function is used to get the whole stable diffusion model with lora layers and freeze non-lora parameters, including Tokenizer, Noise Scheduler, UNet, Text Encoder, and VAE\n",
        "\n",
        "    (2) Arguments:\n",
        "        - pretrained_model_name_or_path: str, model name from Hugging Face\n",
        "        - model_path: str, path to pretrained model.\n",
        "\n",
        "    (3) Returns:\n",
        "        - output: Tokenizer, Noise Scheduler, UNet, Text Encoder, and VAE\n",
        "\n",
        "    \"\"\"\n",
        "    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(\n",
        "        pretrained_model_name_or_path,\n",
        "        subfolder=\"tokenizer\"\n",
        "    )\n",
        "    # text_encoder = CLIPTextModel.from_pretrained(\n",
        "    #     pretrained_model_name_or_path,\n",
        "    #     torch_dtype=weight_dtype,\n",
        "    #     subfolder=\"text_encoder\"\n",
        "    # )\n",
        "    vae = AutoencoderKL.from_pretrained(\n",
        "        pretrained_model_name_or_path,\n",
        "        subfolder=\"vae\"\n",
        "    )\n",
        "    # unet = UNet2DConditionModel.from_pretrained(\n",
        "    #     pretrained_model_name_or_path,\n",
        "    #     torch_dtype=weight_dtype,\n",
        "    #     subfolder=\"unet\"\n",
        "    # )\n",
        "    text_encoder = torch.load(os.path.join(model_path, \"text_encoder.pt\"))\n",
        "    unet = torch.load(os.path.join(model_path, \"unet.pt\"))\n",
        "    vae.requires_grad_(False)\n",
        "    for name, param in unet.named_parameters():\n",
        "        if \"lora\" in name:\n",
        "            param.requires_grad_(True)\n",
        "        else:\n",
        "            param.requires_grad_(False)\n",
        "    for name, param in text_encoder.named_parameters():\n",
        "        if \"lora\" in name:\n",
        "            param.requires_grad_(True)\n",
        "        else:\n",
        "            param.requires_grad_(False)\n",
        "\n",
        "    unet.to(DEVICE, dtype=weight_dtype)\n",
        "    vae.to(DEVICE, dtype=weight_dtype)\n",
        "    text_encoder.to(DEVICE, dtype=weight_dtype)\n",
        "    return tokenizer, noise_scheduler, unet, vae, text_encoder\n",
        "\n",
        "def prepare_optimizer(unet, text_encoder, unet_learning_rate=5e-4, text_encoder_learning_rate=1e-4):\n",
        "    \"\"\"\n",
        "    (1) Goal:\n",
        "        - This function is used to feed trainable parameters from UNet and Text Encoder in to optimizer each with different learning rate\n",
        "\n",
        "    (2) Arguments:\n",
        "        - unet: UNet2DConditionModel, UNet from Hugging Face\n",
        "        - text_encoder: CLIPTextModel, Text Encoder from Hugging Face\n",
        "        - unet_learning_rate: float, learning rate for UNet\n",
        "        - text_encoder_learning_rate: float, learning rate for Text Encoder\n",
        "\n",
        "    (3) Returns:\n",
        "        - output: Optimizer\n",
        "\n",
        "    \"\"\"\n",
        "    unet_lora_layers = list(filter(lambda p: p.requires_grad, unet.parameters()))\n",
        "    text_encoder_lora_layers = list(filter(lambda p: p.requires_grad, text_encoder.parameters()))\n",
        "    trainable_params = [\n",
        "        {\"params\": unet_lora_layers, \"lr\": unet_learning_rate},\n",
        "        {\"params\": text_encoder_lora_layers, \"lr\": text_encoder_learning_rate}\n",
        "    ]\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        trainable_params,\n",
        "        lr=unet_learning_rate,\n",
        "    )\n",
        "    return optimizer\n",
        "\n",
        "def evaluate(pretrained_model_name_or_path, weight_dtype, seed, unet_path, text_encoder_path, validation_prompt, output_folder, train_emb):\n",
        "    \"\"\"\n",
        "    (1) Goal:\n",
        "        - This function is used to evaluate Stable Diffusion by loading UNet and Text Encoder from the given path and calculating face similarity, CLIP score, and the number of faceless images.\n",
        "\n",
        "    (2) Arguments:\n",
        "        - pretrained_model_name_or_path: str, model name from Hugging Face\n",
        "        - weight_dtype: torch.type, model weight type\n",
        "        - seed: int, random seed\n",
        "        - unet_path: str, path to UNet model checkpoint\n",
        "        - text_encoder_path: str, path to Text Encoder model checkpoint\n",
        "        - validation_prompt: list, list of str storing texts for validation\n",
        "        - output_folder: str, directory for saving generated images\n",
        "        - train_emb: tensor, face features of training images\n",
        "\n",
        "    (3) Returns:\n",
        "        - output: face similarity, CLIP score, the number of faceless images\n",
        "\n",
        "    \"\"\"\n",
        "    pipeline = DiffusionPipeline.from_pretrained(\n",
        "        pretrained_model_name_or_path,\n",
        "        torch_dtype=weight_dtype,\n",
        "        safety_checker=None,\n",
        "    )\n",
        "    pipeline.unet = torch.load(unet_path)\n",
        "    pipeline.text_encoder = torch.load(text_encoder_path)\n",
        "    pipeline = pipeline.to(DEVICE)\n",
        "    clip_model_name = \"openai/clip-vit-base-patch32\"\n",
        "    clip_model = AutoModel.from_pretrained(clip_model_name)\n",
        "    clip_processor = AutoProcessor.from_pretrained(clip_model_name)\n",
        "\n",
        "    # run inference\n",
        "    with torch.no_grad():\n",
        "        generator = torch.Generator(device=DEVICE)\n",
        "        generator = generator.manual_seed(seed)\n",
        "        face_score = 0\n",
        "        clip_score = 0\n",
        "        mis = 0\n",
        "        print(\"Generating validaion pictures ......\")\n",
        "        images = []\n",
        "        for i in range(0, len(validation_prompt), 4):\n",
        "            images.extend(pipeline(validation_prompt[i:min(i + 4, len(validation_prompt))], num_inference_steps=30, generator=generator).images)\n",
        "        print(\"Calculating validaion score ......\")\n",
        "        valid_emb = []\n",
        "        for i, image in enumerate(tqdm(images)):\n",
        "            save_file = f\"{output_folder}/valid_image_{i}.png\"\n",
        "            image.save(save_file)\n",
        "            opencvImage = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
        "            emb = DeepFace.represent(\n",
        "                opencvImage,\n",
        "                detector_backend=\"ssd\",\n",
        "                model_name=\"GhostFaceNet\",\n",
        "                enforce_detection=False,\n",
        "            )\n",
        "            if emb == [] or emb[0]['face_confidence'] == 0:\n",
        "                mis += 1\n",
        "                continue\n",
        "            emb = emb[0]\n",
        "            inputs = clip_processor(text=validation_prompt[i], images=image, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                outputs = clip_model(**inputs)\n",
        "            sim = outputs.logits_per_image\n",
        "            clip_score += sim.item()\n",
        "            valid_emb.append(emb['embedding'])\n",
        "        if len(valid_emb) == 0:\n",
        "            return 0, 0, mis\n",
        "        valid_emb = torch.tensor(valid_emb)\n",
        "        valid_emb = (valid_emb / torch.norm(valid_emb, p=2, dim=-1)[:, None]).cuda()\n",
        "        train_emb = (train_emb / torch.norm(train_emb, p=2, dim=-1)[:, None]).cuda()\n",
        "        face_score = torch.cdist(valid_emb, train_emb, p=2).mean().item()\n",
        "        # face_score = torch.min(face_score, 1)[0].mean()\n",
        "        clip_score /= len(validation_prompt) - mis\n",
        "    return face_score, clip_score, mis"
      ],
      "metadata": {
        "id": "3Kuc0_PcHW48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# åŠ è½½é¢„è®­ç»ƒçš„ ResNet æ¨¡å‹\n",
        "resnet = models.resnet50(pretrained=True)\n",
        "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])  # å»æ‰æœ€åçš„å…¨è¿æ¥å±‚\n",
        "resnet.eval()\n",
        "\n",
        "def extract_features(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = train_transform(image).unsqueeze(0)  # æ·»åŠ  batch ç»´åº¦\n",
        "    with torch.no_grad():\n",
        "        features = resnet(image)\n",
        "    return features.squeeze()  # å»æ‰ batch ç»´åº¦\n",
        "\n",
        "# åœ¨ Text2ImageDataset ä¸­ä½¿ç”¨\n",
        "class Text2ImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images_folder, captions_folder, transform, tokenizer):\n",
        "        self.image_paths = []\n",
        "        for ext in IMAGE_EXTENSIONS:\n",
        "            self.image_paths.extend(glob.glob(f\"{images_folder}/*{ext}\"))\n",
        "        self.image_paths = sorted(self.image_paths)\n",
        "        self.train_emb = torch.stack([extract_features(img_path) for img_path in self.image_paths])\n",
        "        caption_paths = sorted(glob.glob(f\"{captions_folder}/*txt\"))\n",
        "        captions = []\n",
        "        for p in caption_paths:\n",
        "            with open(p, \"r\") as f:\n",
        "                captions.append(f.readline())\n",
        "        inputs = tokenizer(\n",
        "            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        self.input_ids = inputs.input_ids\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        input_id = self.input_ids[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            tensor = self.transform(image)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load image path: {img_path}, error: {e}\")\n",
        "            return None\n",
        "        return tensor, input_id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)"
      ],
      "metadata": {
        "id": "8Mg_RUOMLbUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModel, AutoProcessor\n",
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "def evaluate(pretrained_model_name_or_path, weight_dtype, seed, unet_path, text_encoder_path, validation_prompt, output_folder, train_emb):\n",
        "    \"\"\"\n",
        "    (1) Goal:\n",
        "        - This function is used to evaluate Stable Diffusion by loading UNet and Text Encoder from the given path and calculating face similarity, CLIP score, and the number of faceless images.\n",
        "\n",
        "    (2) Arguments:\n",
        "        - pretrained_model_name_or_path: str, model name from Hugging Face\n",
        "        - weight_dtype: torch.type, model weight type\n",
        "        - seed: int, random seed\n",
        "        - unet_path: str, path to UNet model checkpoint\n",
        "        - text_encoder_path: str, path to Text Encoder model checkpoint\n",
        "        - validation_prompt: list, list of str storing texts for validation\n",
        "        - output_folder: str, directory for saving generated images\n",
        "        - train_emb: tensor, face features of training images\n",
        "\n",
        "    (3) Returns:\n",
        "        - output: face similarity, CLIP score, the number of faceless images\n",
        "    \"\"\"\n",
        "    # Load ResNet50 model for feature extraction\n",
        "    resnet = models.resnet50(pretrained=True)\n",
        "    resnet = torch.nn.Sequential(*list(resnet.children())[:-1])  # Remove the final classification layer\n",
        "    resnet = resnet.to(DEVICE)\n",
        "    resnet.eval()\n",
        "\n",
        "    # Image preprocessing for ResNet50\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    # Load Stable Diffusion pipeline\n",
        "    pipeline = DiffusionPipeline.from_pretrained(\n",
        "        pretrained_model_name_or_path,\n",
        "        torch_dtype=weight_dtype,\n",
        "        safety_checker=None,\n",
        "    )\n",
        "    pipeline.unet = torch.load(unet_path)\n",
        "    pipeline.text_encoder = torch.load(text_encoder_path)\n",
        "    pipeline = pipeline.to(DEVICE)\n",
        "\n",
        "    # Load CLIP model for CLIP score\n",
        "    clip_model_name = \"openai/clip-vit-base-patch32\"\n",
        "    clip_model = AutoModel.from_pretrained(clip_model_name)\n",
        "    clip_processor = AutoProcessor.from_pretrained(clip_model_name)\n",
        "\n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        generator = torch.Generator(device=DEVICE)\n",
        "        generator = generator.manual_seed(seed)\n",
        "        face_score = 0\n",
        "        clip_score = 0\n",
        "        mis = 0\n",
        "        print(\"Generating validation pictures ......\")\n",
        "        images = []\n",
        "        for i in range(0, len(validation_prompt), 4):\n",
        "            images.extend(pipeline(validation_prompt[i:min(i + 4, len(validation_prompt))], num_inference_steps=30, generator=generator).images)\n",
        "        print(\"Calculating validation score ......\")\n",
        "        valid_emb = []\n",
        "        for i, image in enumerate(tqdm(images)):\n",
        "            save_file = f\"{output_folder}/valid_image_{i}.png\"\n",
        "            image.save(save_file)\n",
        "\n",
        "            # Preprocess image for ResNet50\n",
        "            image_tensor = preprocess(image).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "            # Extract features using ResNet50\n",
        "            with torch.no_grad():\n",
        "                emb = resnet(image_tensor).squeeze().cpu().numpy()\n",
        "\n",
        "            # Check if the image contains a face\n",
        "            if np.linalg.norm(emb) < 1e-6:  # Simple threshold to detect faceless images\n",
        "                mis += 1\n",
        "                continue\n",
        "\n",
        "            # Calculate CLIP score\n",
        "            inputs = clip_processor(text=validation_prompt[i], images=image, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                outputs = clip_model(**inputs)\n",
        "            sim = outputs.logits_per_image\n",
        "            clip_score += sim.item()\n",
        "\n",
        "            # Store valid embeddings\n",
        "            valid_emb.append(emb)\n",
        "\n",
        "        if len(valid_emb) == 0:\n",
        "            return 0, 0, mis\n",
        "\n",
        "        # Normalize embeddings\n",
        "        valid_emb = torch.tensor(valid_emb)\n",
        "        valid_emb = (valid_emb / torch.norm(valid_emb, p=2, dim=-1)[:, None]).to(DEVICE)\n",
        "        train_emb = (train_emb / torch.norm(train_emb, p=2, dim=-1)[:, None]).to(DEVICE)\n",
        "\n",
        "        # Calculate face similarity\n",
        "        face_score = torch.cdist(valid_emb, train_emb, p=2).mean().item()\n",
        "        clip_score /= len(validation_prompt) - mis\n",
        "\n",
        "    return face_score, clip_score, mis"
      ],
      "metadata": {
        "id": "3PtZMrzRSNL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Dataset, LoRA model, and Optimizer\n",
        "Declare everything needed for Stable Diffusion fine-tuning."
      ],
      "metadata": {
        "id": "xJcZwF4NM5xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, noise_scheduler, unet, vae, text_encoder = prepare_lora_model(pretrained_model_name_or_path, model_path)\n",
        "optimizer                                           = prepare_optimizer(unet, text_encoder, unet_learning_rate, text_encoder_learning_rate)\n",
        "lr_scheduler = get_scheduler(\n",
        "    lr_scheduler_name,\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=lr_warmup_steps,\n",
        "    num_training_steps=max_train_steps,\n",
        "    num_cycles=3\n",
        ")\n",
        "\n",
        "dataset = Text2ImageDataset(\n",
        "    images_folder=images_folder,\n",
        "    captions_folder=captions_folder,\n",
        "    transform=train_transform,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "def collate_fn(examples):\n",
        "    pixel_values = []\n",
        "    input_ids = []\n",
        "    for tensor, input_id in examples:\n",
        "        pixel_values.append(tensor)\n",
        "        input_ids.append(input_id)\n",
        "    pixel_values = torch.stack(pixel_values, dim=0).float()\n",
        "    input_ids = torch.stack(input_ids, dim=0)\n",
        "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    batch_size=train_batch_size,\n",
        "    num_workers=8,\n",
        ")\n",
        "print(\"Preparation Finished!\")"
      ],
      "metadata": {
        "id": "mqSmWcCUOnVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "etTIXiZOUFmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Fine-tuning\n",
        "This cell takes 25 minutes to run in the default setting, but it may vary depending on the condition of Colab and `max_train_step`."
      ],
      "metadata": {
        "id": "jBGv8WXnNDap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "progress_bar = tqdm(\n",
        "    range(0, max_train_steps),\n",
        "    initial=0,\n",
        "    desc=\"Steps\",\n",
        ")\n",
        "global_step = 0\n",
        "num_epochs = math.ceil(max_train_steps / len(train_dataloader))\n",
        "validation_step = int(max_train_steps * validation_step_ratio)\n",
        "best_face_score = float(\"inf\")\n",
        "for epoch in range(num_epochs):\n",
        "    unet.train()\n",
        "    text_encoder.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if global_step >= max_train_steps:\n",
        "            break\n",
        "        latents = vae.encode(batch[\"pixel_values\"].to(DEVICE, dtype=weight_dtype)).latent_dist.sample()\n",
        "        latents = latents * vae.config.scaling_factor\n",
        "        # Sample noise that we'll add to the latents\n",
        "        noise = torch.randn_like(latents)\n",
        "        bsz = latents.shape[0]\n",
        "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
        "        timesteps = timesteps.long()\n",
        "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "        # Get the text embedding for conditioning\n",
        "        encoder_hidden_states = text_encoder(batch[\"input_ids\"].to(latents.device), return_dict=False)[0]\n",
        "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "            target = noise\n",
        "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "            target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
        "        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]\n",
        "        if not snr_gamma:\n",
        "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
        "        else:\n",
        "            snr = compute_snr(noise_scheduler, timesteps)\n",
        "            mse_loss_weights = torch.stack([snr, snr_gamma * torch.ones_like(timesteps)], dim=1).min(\n",
        "                dim=1\n",
        "            )[0]\n",
        "            if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "                mse_loss_weights = mse_loss_weights / snr\n",
        "            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "                mse_loss_weights = mse_loss_weights / (snr + 1)\n",
        "\n",
        "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n",
        "            loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\n",
        "            loss = loss.mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "        global_step += 1\n",
        "\n",
        "        if global_step % validation_step == 0 or global_step == max_train_steps:\n",
        "            save_path = os.path.join(output_folder, f\"checkpoint-last\")\n",
        "            unet_path = os.path.join(save_path, \"unet.pt\")\n",
        "            text_encoder_path = os.path.join(save_path, \"text_encoder.pt\")\n",
        "            print(f\"Saving Checkpoint to {save_path} ......\")\n",
        "            os.makedirs(save_path, exist_ok=True)\n",
        "            torch.save(unet, unet_path)\n",
        "            torch.save(text_encoder, text_encoder_path)\n",
        "            save_path = os.path.join(output_folder, f\"checkpoint-{global_step + 1000}\")\n",
        "            os.makedirs(save_path, exist_ok=True)\n",
        "            face_score, clip_score, mis = evaluate(\n",
        "                pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "                weight_dtype=weight_dtype,\n",
        "                seed=seed,\n",
        "                unet_path=unet_path,\n",
        "                text_encoder_path=text_encoder_path,\n",
        "                validation_prompt=validation_prompt[:validation_prompt_num],\n",
        "                output_folder=save_path,\n",
        "                train_emb=dataset.train_emb\n",
        "            )\n",
        "            print(\"Step:\", global_step, \"Face Similarity Score:\", face_score, \"CLIP Score:\", clip_score, \"Faceless Images:\", mis)\n",
        "            if face_score < best_face_score:\n",
        "                best_face_score = face_score\n",
        "                save_path = os.path.join(output_folder, f\"checkpoint-best\")\n",
        "                unet_path = os.path.join(save_path, \"unet.pt\")\n",
        "                text_encoder_path = os.path.join(save_path, \"text_encoder.pt\")\n",
        "                os.makedirs(save_path, exist_ok=True)\n",
        "                torch.save(unet, unet_path)\n",
        "                torch.save(text_encoder, text_encoder_path)\n",
        "print(\"Fine-tuning Finished!!!\")"
      ],
      "metadata": {
        "id": "TDb7p8O9OqLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# åŠ è½½é¢„è®­ç»ƒçš„ ResNet50 æ¨¡å‹\n",
        "resnet = models.resnet50(pretrained=True)\n",
        "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])  # ç§»é™¤æœ€åçš„å…¨è¿æ¥å±‚\n",
        "resnet = resnet.to(DEVICE)\n",
        "resnet.eval()\n",
        "\n",
        "# å›¾åƒé¢„å¤„ç†\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# æå–ç‰¹å¾\n",
        "def extract_features(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = preprocess(image).unsqueeze(0).to(DEVICE)  # æ·»åŠ  batch ç»´åº¦\n",
        "    with torch.no_grad():\n",
        "        features = resnet(image)\n",
        "    return features.squeeze().cpu().numpy()  # å»æ‰ batch ç»´åº¦å¹¶è½¬æ¢ä¸º NumPy æ•°ç»„\n",
        "\n",
        "# æå–è®­ç»ƒå›¾åƒçš„ç‰¹å¾\n",
        "# train_emb = torch.tensor([extract_features(img_path) for img_path in train_image_paths])"
      ],
      "metadata": {
        "id": "bjQOKZ8Am_y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing\n",
        "The fine-tuning process is done. We then want to test our model.\n",
        "\n",
        "We will first load the fine-tuned model for checkpoint we saved and calculate the face similarity, CLIP score, and the number of faceless images.\n",
        "This process will take about 15 minutes."
      ],
      "metadata": {
        "id": "QpZ83eq-NOD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = os.path.join(output_folder, f\"checkpoint-last\") # è¨­å®šä½¿ç”¨å“ªå€‹checkpoint inference\n",
        "unet_path = os.path.join(checkpoint_path, \"unet.pt\")\n",
        "text_encoder_path = os.path.join(checkpoint_path, \"text_encoder.pt\")\n",
        "inference_path = os.path.join(project_dir, \"inference\")\n",
        "os.makedirs(inference_path, exist_ok=True)\n",
        "train_image_paths = []\n",
        "for ext in IMAGE_EXTENSIONS:\n",
        "    train_image_paths.extend(glob.glob(f\"{images_folder}/*{ext}\"))\n",
        "train_image_paths = sorted(train_image_paths)\n",
        "# train_emb = torch.tensor([DeepFace.represent(img_path, detector_backend=\"ssd\", model_name=\"GhostFaceNet\", enforce_detection=False)[0]['embedding'] for img_path in train_image_paths])\n",
        "train_emb = torch.tensor([extract_features(img_path) for img_path in train_image_paths])\n",
        "face_score, clip_score, mis = evaluate(\n",
        "    pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "    weight_dtype=weight_dtype,\n",
        "    seed=seed,\n",
        "    unet_path=unet_path,\n",
        "    text_encoder_path=text_encoder_path,\n",
        "    validation_prompt=validation_prompt,\n",
        "    output_folder=inference_path,\n",
        "    train_emb=train_emb,\n",
        ")\n",
        "print(\"Face Similarity Score:\", face_score, \"CLIP Score:\", clip_score, \"Faceless Images:\", mis)"
      ],
      "metadata": {
        "id": "r0WdWhGnZVnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f0YYxsTva0AN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}