{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "xJcZwF4NM5xx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luoclab/Hung-yi-Lee-HomeWork/blob/main/GenerativeAI2024spring/GenAI_HW10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GenAI HW10: Stable Diffusion Fine-tuning\n",
        "In this homework, you will fine-tune your own Stable Diffusion model to generate your customized images from given text description. For more details, please refer to homework slides\n",
        "\n",
        "## **TODOs**\n",
        "\n",
        "1. Read the slides and make sure you know the objectives of this homework.\n",
        "2. Save a copy of this Colab notebook.\n",
        "3. Follow the steps in this Colab notebook to fine-tune your Stable Diffusion.\n",
        "4. Evaluate outputs using FaceNet and CLIP\n",
        "5. Update results and datasets to NTU COOL\n",
        "If you have any questions, please contact the TAs via TA hours, NTU COOL, or email to ntu-gen-ai-2024-spring-ta@googlegroups.com\n",
        "\n",
        "Tips: At the top of each cell, it shows whether you need to change hyperparameters in that cell and how long it might take.\n",
        "\n",
        "This is based on the work of [Hugging Face](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py).\n",
        "\n",
        "And special thanks to [Celebrity Face Image Dataset](https://www.kaggle.com/datasets/vishesh1412/celebrity-face-image-dataset).\n",
        "\n",
        "\n",
        "Thank you!\n"
      ],
      "metadata": {
        "id": "CnJtiRaRuTFX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "uNjz10u8biUn",
        "outputId": "613afc06-3c31-4874-dc73-7ae1f5760234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Connecting to Google Drive...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-76fa14743ab7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üìÇ Connecting to Google Drive...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mproject_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject_name\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproject_name\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mproject_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mproject_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "COLAB = True\n",
        "\n",
        "if COLAB:\n",
        "    from google.colab.output import clear as clear_output\n",
        "else:\n",
        "    from IPython.display import clear_output\n",
        "\n",
        "\n",
        "#@markdown ## Link to Google Drive\n",
        "#@markdown This cell will load some requirements and create the necessary folders in your Google Drive. <p>\n",
        "#@markdown Your project name can't contain spaces but it can contain a single / to make a subfolder in your dataset.\n",
        "project_name = \"Brad\" #@param {type:\"string\"}\n",
        "project_name = project_name.strip()\n",
        "# dataset_name = \"Brad-512\" #@param [\"Brad-512\", \"Anne-512\"]\n",
        "dataset_name = \"Brad\"\n",
        "\n",
        "if not project_name or any(c in project_name for c in \" .()\\\"'\\\\\") or project_name.count(\"/\") > 1:\n",
        "    print(\"Please write a valid project_name.\")\n",
        "else:\n",
        "    if COLAB and not os.path.exists('/content/drive'):\n",
        "      from google.colab import drive\n",
        "      print(\"üìÇ Connecting to Google Drive...\")\n",
        "      drive.mount('/content/drive')\n",
        "\n",
        "    project_base = project_name if \"/\" not in project_name else project_name[:project_name.rfind(\"/\")]\n",
        "    project_subfolder = project_name if \"/\" not in project_name else project_name[project_name.rfind(\"/\")+1:]\n",
        "\n",
        "    root_dir = \"/content\" if COLAB else \"~/Loras\"\n",
        "    main_dir        = os.path.join(root_dir, \"drive/MyDrive/GenAI-HW10\") if COLAB else root_dir\n",
        "    project_dir =  os.path.join(main_dir, project_name)\n",
        "    os.makedirs(main_dir, exist_ok=True)\n",
        "    os.makedirs(project_dir, exist_ok=True)\n",
        "    zip_file = os.path.join(main_dir, \"Datasets.zip\")\n",
        "    !gdown 1OXPG2vNb8bG2334HML8vKpqo8UbAEV3d -O {zip_file}\n",
        "    !unzip -q -o {zip_file} -d {main_dir}\n",
        "    log_file = os.path.join(project_dir, \"logs.zip\")\n",
        "    log_dir = os.path.join(project_dir, \"logs\")\n",
        "    !git clone https://huggingface.co/yahcreeper/GenAI-HW10-Model {log_dir}\n",
        "    # !cd {log_dir}\n",
        "    # !git lfs pull\n",
        "    # !gdown 1kalT3k7kEV0xcD6pf_OTSo7npHmsfZ_z -O {log_file}\n",
        "    # !unzip -q -o {log_file} -d {project_dir}\n",
        "    # !rm -f {log_file}\n",
        "    model_path = os.path.join(project_dir, \"logs\", \"checkpoint-last\")\n",
        "    images_folder   = os.path.join(main_dir, \"Datasets\", dataset_name)\n",
        "    prompts_folder  = os.path.join(main_dir, \"Datasets\", \"prompts\")\n",
        "    captions_folder = images_folder\n",
        "    os.makedirs(images_folder, exist_ok=True)\n",
        "\n",
        "    print(f\"‚úÖ Project {project_name} is ready!\")\n",
        "    step1_installed_flag = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##  Install the required packages\n",
        "#@markdown In this session, we will install some well-established packages to facilitate the fine-tuning process. <p>\n",
        "#@markdown The installation will take about 5 minutes.\n",
        "os.chdir(root_dir)\n",
        "!pip -q install torch==2.5 timm==1.0.7 fairscale==0.4.13 transformers==4.41.2 requests accelerate==0.31.0 diffusers==0.29.1 einop==0.0.1 safetensors==0.4.3 voluptuous==0.15.1 jax peft==0.11.1 deepface==0.0.92 tensorflow==2.15.0 keras==2.15.0"
      ],
      "metadata": {
        "id": "lJ4pB6-_dC91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##  Import necessary packages\n",
        "#@markdown It is recommmended NOT to change codes in this cell.\n",
        "import argparse\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import glob\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "import transformers\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "from tqdm.auto import tqdm\n",
        "from peft import LoraConfig\n",
        "from peft.utils import get_peft_model_state_dict\n",
        "from transformers import AutoProcessor, AutoModel, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "import diffusers\n",
        "from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, StableDiffusionPipeline, UNet2DConditionModel\n",
        "from diffusers.optimization import get_scheduler\n",
        "from diffusers.utils import convert_state_dict_to_diffusers\n",
        "from diffusers.training_utils import compute_snr\n",
        "from diffusers.utils.torch_utils import is_compiled_module\n",
        "from deepface import DeepFace\n",
        "import cv2"
      ],
      "metadata": {
        "id": "pusB39oYsycq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change the following parameters, or the process may crashed due to GPU out of memory.\n",
        "output_folder = os.path.join(project_dir, \"logs\") # Â≠òÊîæmodel checkpointsË∑üvalidationÁµêÊûúÁöÑË≥áÊñôÂ§æ\n",
        "seed = 1126 # random seed\n",
        "train_batch_size = 2 # training batch size\n",
        "resolution = 512 # Image size\n",
        "weight_dtype = torch.bfloat16 #\n",
        "snr_gamma = 5\n",
        "#####\n",
        "\n",
        "#@markdown ## Important parameters for fine-tuning Stable Diffusion\n",
        "pretrained_model_name_or_path = \"stablediffusionapi/cyberrealistic-41\"\n",
        "lora_rank = 32\n",
        "lora_alpha = 16\n",
        "#@markdown ### ‚ñ∂Ô∏è Learning Rate\n",
        "#@markdown The learning rate is the most important for your results. If you want to train slower with lots of images, or if your dim and alpha are high, move the unet to 2e-4 or lower. <p>\n",
        "#@markdown The text encoder helps your Lora learn concepts slightly better. It is recommended to make it half or a fifth of the unet. If you're training a style you can even set it to 0.\n",
        "learning_rate = 1e-4 #@param {type:\"number\"}\n",
        "unet_learning_rate = learning_rate\n",
        "text_encoder_learning_rate = learning_rate\n",
        "lr_scheduler_name = \"cosine_with_restarts\" # Ë®≠ÂÆöÂ≠∏ÁøíÁéáÁöÑÊéíÁ®ã\n",
        "lr_warmup_steps = 100 # Ë®≠ÂÆöÁ∑©ÊÖ¢Êõ¥Êñ∞ÁöÑÊ≠•Êï∏\n",
        "#@markdown ### ‚ñ∂Ô∏è Steps\n",
        "#@markdown Choose your training step and the number of generated images per each validaion\n",
        "max_train_steps = 200 #@param {type:\"slider\", min:200, max:2000, step:100}\n",
        "validation_prompt = \"validation_prompt.txt\"\n",
        "validation_prompt_path = os.path.join(prompts_folder, validation_prompt)\n",
        "validation_prompt_num = 3 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "validation_step_ratio = 1 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "with open(validation_prompt_path, \"r\") as f:\n",
        "    validation_prompt = [line.strip() for line in f.readlines()]\n",
        "#####"
      ],
      "metadata": {
        "id": "oTbop_tCsxIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Some Useful Functions and Class"
      ],
      "metadata": {
        "id": "bpPjMo_IMtt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "IMAGE_EXTENSIONS = [\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".PNG\", \".JPG\", \".JPEG\", \".WEBP\", \".BMP\"]\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "        transforms.CenterCrop(resolution),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "    ]\n",
        ")\n",
        "class Text2ImageDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    (1) Goal:\n",
        "        - This class is used to build dataset for finetuning text-to-image model\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, images_folder, captions_folder, transform, tokenizer):\n",
        "        \"\"\"\n",
        "        (2) Arguments:\n",
        "            - images_folder: str, path to images\n",
        "            - captions_folder: str, path to captions\n",
        "            - transform: function, turn raw image into torch.tensor\n",
        "            - tokenizer: CLIPTokenize, turn sentences into word ids\n",
        "        \"\"\"\n",
        "        self.image_paths = []\n",
        "        for ext in IMAGE_EXTENSIONS:\n",
        "            self.image_paths.extend(glob.glob(f\"{images_folder}/*{ext}\"))\n",
        "        self.image_paths = sorted(self.image_paths)\n",
        "        self.train_emb = torch.tensor([DeepFace.represent(img_path, detector_backend=\"ssd\", model_name=\"GhostFaceNet\", enforce_detection=False)[0]['embedding'] for img_path in self.image_paths])\n",
        "        caption_paths = sorted(glob.glob(f\"{captions_folder}/*txt\"))\n",
        "        captions = []\n",
        "        for p in caption_paths:\n",
        "            with open(p, \"r\") as f:\n",
        "                captions.append(f.readline())\n",
        "        inputs = tokenizer(\n",
        "            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        self.input_ids = inputs.input_ids\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        input_id = self.input_ids[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            # convert to tensor temporarily so dataloader will accept it\n",
        "            tensor = self.transform(image)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load image path: {img_path}, error: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "        return tensor, input_id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "\n",
        "def prepare_lora_model(pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\", model_path=None):\n",
        "    \"\"\"\n",
        "    (1) Goal:\n",
        "        - This function is used to get the whole stable diffusion model with lora layers and freeze non-lora parameters, including Tokenizer, Noise Scheduler, UNet, Text Encoder, and VAE\n",
        "\n",
        "    (2) Arguments:\n",
        "        - pretrained_model_name_or_path: str, model name from Hugging Face\n",
        "        - model_path: str, path to pretrained model.\n",
        "\n",
        "    (3) Returns:\n",
        "        - output: Tokenizer, Noise Scheduler, UNet, Text Encoder, and VAE\n",
        "\n",
        "    \"\"\"\n",
        "    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(\n",
        "        pretrained_model_name_or_path,\n",
        "        subfolder=\"tokenizer\"\n",
        "    )\n",
        "    # text_encoder = CLIPTextModel.from_pretrained(\n",
        "    #     pretrained_model_name_or_path,\n",
        "    #     torch_dtype=weight_dtype,\n",
        "    #     subfolder=\"text_encoder\"\n",
        "    # )\n",
        "    vae = AutoencoderKL.from_pretrained(\n",
        "        pretrained_model_name_or_path,\n",
        "        subfolder=\"vae\"\n",
        "    )\n",
        "    # unet = UNet2DConditionModel.from_pretrained(\n",
        "    #     pretrained_model_name_or_path,\n",
        "    #     torch_dtype=weight_dtype,\n",
        "    #     subfolder=\"unet\"\n",
        "    # )\n",
        "    text_encoder = torch.load(os.path.join(model_path, \"text_encoder.pt\"))\n",
        "    unet = torch.load(os.path.join(model_path, \"unet.pt\"))\n",
        "    vae.requires_grad_(False)\n",
        "    for name, param in unet.named_parameters():\n",
        "        if \"lora\" in name:\n",
        "            param.requires_grad_(True)\n",
        "        else:\n",
        "            param.requires_grad_(False)\n",
        "    for name, param in text_encoder.named_parameters():\n",
        "        if \"lora\" in name:\n",
        "            param.requires_grad_(True)\n",
        "        else:\n",
        "            param.requires_grad_(False)\n",
        "\n",
        "    unet.to(DEVICE, dtype=weight_dtype)\n",
        "    vae.to(DEVICE, dtype=weight_dtype)\n",
        "    text_encoder.to(DEVICE, dtype=weight_dtype)\n",
        "    return tokenizer, noise_scheduler, unet, vae, text_encoder\n",
        "\n",
        "def prepare_optimizer(unet, text_encoder, unet_learning_rate=5e-4, text_encoder_learning_rate=1e-4):\n",
        "    \"\"\"\n",
        "    (1) Goal:\n",
        "        - This function is used to feed trainable parameters from UNet and Text Encoder in to optimizer each with different learning rate\n",
        "\n",
        "    (2) Arguments:\n",
        "        - unet: UNet2DConditionModel, UNet from Hugging Face\n",
        "        - text_encoder: CLIPTextModel, Text Encoder from Hugging Face\n",
        "        - unet_learning_rate: float, learning rate for UNet\n",
        "        - text_encoder_learning_rate: float, learning rate for Text Encoder\n",
        "\n",
        "    (3) Returns:\n",
        "        - output: Optimizer\n",
        "\n",
        "    \"\"\"\n",
        "    unet_lora_layers = list(filter(lambda p: p.requires_grad, unet.parameters()))\n",
        "    text_encoder_lora_layers = list(filter(lambda p: p.requires_grad, text_encoder.parameters()))\n",
        "    trainable_params = [\n",
        "        {\"params\": unet_lora_layers, \"lr\": unet_learning_rate},\n",
        "        {\"params\": text_encoder_lora_layers, \"lr\": text_encoder_learning_rate}\n",
        "    ]\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        trainable_params,\n",
        "        lr=unet_learning_rate,\n",
        "    )\n",
        "    return optimizer\n",
        "\n",
        "def evaluate(pretrained_model_name_or_path, weight_dtype, seed, unet_path, text_encoder_path, validation_prompt, output_folder, train_emb):\n",
        "    \"\"\"\n",
        "    (1) Goal:\n",
        "        - This function is used to evaluate Stable Diffusion by loading UNet and Text Encoder from the given path and calculating face similarity, CLIP score, and the number of faceless images.\n",
        "\n",
        "    (2) Arguments:\n",
        "        - pretrained_model_name_or_path: str, model name from Hugging Face\n",
        "        - weight_dtype: torch.type, model weight type\n",
        "        - seed: int, random seed\n",
        "        - unet_path: str, path to UNet model checkpoint\n",
        "        - text_encoder_path: str, path to Text Encoder model checkpoint\n",
        "        - validation_prompt: list, list of str storing texts for validation\n",
        "        - output_folder: str, directory for saving generated images\n",
        "        - train_emb: tensor, face features of training images\n",
        "\n",
        "    (3) Returns:\n",
        "        - output: face similarity, CLIP score, the number of faceless images\n",
        "\n",
        "    \"\"\"\n",
        "    pipeline = DiffusionPipeline.from_pretrained(\n",
        "        pretrained_model_name_or_path,\n",
        "        torch_dtype=weight_dtype,\n",
        "        safety_checker=None,\n",
        "    )\n",
        "    pipeline.unet = torch.load(unet_path)\n",
        "    pipeline.text_encoder = torch.load(text_encoder_path)\n",
        "    pipeline = pipeline.to(DEVICE)\n",
        "    clip_model_name = \"openai/clip-vit-base-patch32\"\n",
        "    clip_model = AutoModel.from_pretrained(clip_model_name)\n",
        "    clip_processor = AutoProcessor.from_pretrained(clip_model_name)\n",
        "\n",
        "    # run inference\n",
        "    with torch.no_grad():\n",
        "        generator = torch.Generator(device=DEVICE)\n",
        "        generator = generator.manual_seed(seed)\n",
        "        face_score = 0\n",
        "        clip_score = 0\n",
        "        mis = 0\n",
        "        print(\"Generating validaion pictures ......\")\n",
        "        images = []\n",
        "        for i in range(0, len(validation_prompt), 4):\n",
        "            images.extend(pipeline(validation_prompt[i:min(i + 4, len(validation_prompt))], num_inference_steps=30, generator=generator).images)\n",
        "        print(\"Calculating validaion score ......\")\n",
        "        valid_emb = []\n",
        "        for i, image in enumerate(tqdm(images)):\n",
        "            save_file = f\"{output_folder}/valid_image_{i}.png\"\n",
        "            image.save(save_file)\n",
        "            opencvImage = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
        "            emb = DeepFace.represent(\n",
        "                opencvImage,\n",
        "                detector_backend=\"ssd\",\n",
        "                model_name=\"GhostFaceNet\",\n",
        "                enforce_detection=False,\n",
        "            )\n",
        "            if emb == [] or emb[0]['face_confidence'] == 0:\n",
        "                mis += 1\n",
        "                continue\n",
        "            emb = emb[0]\n",
        "            inputs = clip_processor(text=validation_prompt[i], images=image, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                outputs = clip_model(**inputs)\n",
        "            sim = outputs.logits_per_image\n",
        "            clip_score += sim.item()\n",
        "            valid_emb.append(emb['embedding'])\n",
        "        if len(valid_emb) == 0:\n",
        "            return 0, 0, mis\n",
        "        valid_emb = torch.tensor(valid_emb)\n",
        "        valid_emb = (valid_emb / torch.norm(valid_emb, p=2, dim=-1)[:, None]).cuda()\n",
        "        train_emb = (train_emb / torch.norm(train_emb, p=2, dim=-1)[:, None]).cuda()\n",
        "        face_score = torch.cdist(valid_emb, train_emb, p=2).mean().item()\n",
        "        # face_score = torch.min(face_score, 1)[0].mean()\n",
        "        clip_score /= len(validation_prompt) - mis\n",
        "    return face_score, clip_score, mis"
      ],
      "metadata": {
        "id": "3Kuc0_PcHW48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÁöÑ ResNet Ê®°Âûã\n",
        "resnet = models.resnet50(pretrained=True)\n",
        "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])  # ÂéªÊéâÊúÄÂêéÁöÑÂÖ®ËøûÊé•Â±Ç\n",
        "resnet.eval()\n",
        "\n",
        "def extract_features(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = train_transform(image).unsqueeze(0)  # Ê∑ªÂä† batch Áª¥Â∫¶\n",
        "    with torch.no_grad():\n",
        "        features = resnet(image)\n",
        "    return features.squeeze()  # ÂéªÊéâ batch Áª¥Â∫¶\n",
        "\n",
        "# Âú® Text2ImageDataset ‰∏≠‰ΩøÁî®\n",
        "class Text2ImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images_folder, captions_folder, transform, tokenizer):\n",
        "        self.image_paths = []\n",
        "        for ext in IMAGE_EXTENSIONS:\n",
        "            self.image_paths.extend(glob.glob(f\"{images_folder}/*{ext}\"))\n",
        "        self.image_paths = sorted(self.image_paths)\n",
        "        self.train_emb = torch.stack([extract_features(img_path) for img_path in self.image_paths])\n",
        "        caption_paths = sorted(glob.glob(f\"{captions_folder}/*txt\"))\n",
        "        captions = []\n",
        "        for p in caption_paths:\n",
        "            with open(p, \"r\") as f:\n",
        "                captions.append(f.readline())\n",
        "        inputs = tokenizer(\n",
        "            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        self.input_ids = inputs.input_ids\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        input_id = self.input_ids[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            tensor = self.transform(image)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load image path: {img_path}, error: {e}\")\n",
        "            return None\n",
        "        return tensor, input_id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)"
      ],
      "metadata": {
        "id": "8Mg_RUOMLbUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModel, AutoProcessor\n",
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "def evaluate(pretrained_model_name_or_path, weight_dtype, seed, unet_path, text_encoder_path, validation_prompt, output_folder, train_emb):\n",
        "    \"\"\"\n",
        "    (1) Goal:\n",
        "        - This function is used to evaluate Stable Diffusion by loading UNet and Text Encoder from the given path and calculating face similarity, CLIP score, and the number of faceless images.\n",
        "\n",
        "    (2) Arguments:\n",
        "        - pretrained_model_name_or_path: str, model name from Hugging Face\n",
        "        - weight_dtype: torch.type, model weight type\n",
        "        - seed: int, random seed\n",
        "        - unet_path: str, path to UNet model checkpoint\n",
        "        - text_encoder_path: str, path to Text Encoder model checkpoint\n",
        "        - validation_prompt: list, list of str storing texts for validation\n",
        "        - output_folder: str, directory for saving generated images\n",
        "        - train_emb: tensor, face features of training images\n",
        "\n",
        "    (3) Returns:\n",
        "        - output: face similarity, CLIP score, the number of faceless images\n",
        "    \"\"\"\n",
        "    # Load ResNet50 model for feature extraction\n",
        "    resnet = models.resnet50(pretrained=True)\n",
        "    resnet = torch.nn.Sequential(*list(resnet.children())[:-1])  # Remove the final classification layer\n",
        "    resnet = resnet.to(DEVICE)\n",
        "    resnet.eval()\n",
        "\n",
        "    # Image preprocessing for ResNet50\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    # Load Stable Diffusion pipeline\n",
        "    pipeline = DiffusionPipeline.from_pretrained(\n",
        "        pretrained_model_name_or_path,\n",
        "        torch_dtype=weight_dtype,\n",
        "        safety_checker=None,\n",
        "    )\n",
        "    pipeline.unet = torch.load(unet_path)\n",
        "    pipeline.text_encoder = torch.load(text_encoder_path)\n",
        "    pipeline = pipeline.to(DEVICE)\n",
        "\n",
        "    # Load CLIP model for CLIP score\n",
        "    clip_model_name = \"openai/clip-vit-base-patch32\"\n",
        "    clip_model = AutoModel.from_pretrained(clip_model_name)\n",
        "    clip_processor = AutoProcessor.from_pretrained(clip_model_name)\n",
        "\n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        generator = torch.Generator(device=DEVICE)\n",
        "        generator = generator.manual_seed(seed)\n",
        "        face_score = 0\n",
        "        clip_score = 0\n",
        "        mis = 0\n",
        "        print(\"Generating validation pictures ......\")\n",
        "        images = []\n",
        "        for i in range(0, len(validation_prompt), 4):\n",
        "            images.extend(pipeline(validation_prompt[i:min(i + 4, len(validation_prompt))], num_inference_steps=30, generator=generator).images)\n",
        "        print(\"Calculating validation score ......\")\n",
        "        valid_emb = []\n",
        "        for i, image in enumerate(tqdm(images)):\n",
        "            save_file = f\"{output_folder}/valid_image_{i}.png\"\n",
        "            image.save(save_file)\n",
        "\n",
        "            # Preprocess image for ResNet50\n",
        "            image_tensor = preprocess(image).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "            # Extract features using ResNet50\n",
        "            with torch.no_grad():\n",
        "                emb = resnet(image_tensor).squeeze().cpu().numpy()\n",
        "\n",
        "            # Check if the image contains a face\n",
        "            if np.linalg.norm(emb) < 1e-6:  # Simple threshold to detect faceless images\n",
        "                mis += 1\n",
        "                continue\n",
        "\n",
        "            # Calculate CLIP score\n",
        "            inputs = clip_processor(text=validation_prompt[i], images=image, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                outputs = clip_model(**inputs)\n",
        "            sim = outputs.logits_per_image\n",
        "            clip_score += sim.item()\n",
        "\n",
        "            # Store valid embeddings\n",
        "            valid_emb.append(emb)\n",
        "\n",
        "        if len(valid_emb) == 0:\n",
        "            return 0, 0, mis\n",
        "\n",
        "        # Normalize embeddings\n",
        "        valid_emb = torch.tensor(valid_emb)\n",
        "        valid_emb = (valid_emb / torch.norm(valid_emb, p=2, dim=-1)[:, None]).to(DEVICE)\n",
        "        train_emb = (train_emb / torch.norm(train_emb, p=2, dim=-1)[:, None]).to(DEVICE)\n",
        "\n",
        "        # Calculate face similarity\n",
        "        face_score = torch.cdist(valid_emb, train_emb, p=2).mean().item()\n",
        "        clip_score /= len(validation_prompt) - mis\n",
        "\n",
        "    return face_score, clip_score, mis"
      ],
      "metadata": {
        "id": "3PtZMrzRSNL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Dataset, LoRA model, and Optimizer\n",
        "Declare everything needed for Stable Diffusion fine-tuning."
      ],
      "metadata": {
        "id": "xJcZwF4NM5xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, noise_scheduler, unet, vae, text_encoder = prepare_lora_model(pretrained_model_name_or_path, model_path)\n",
        "optimizer                                           = prepare_optimizer(unet, text_encoder, unet_learning_rate, text_encoder_learning_rate)\n",
        "lr_scheduler = get_scheduler(\n",
        "    lr_scheduler_name,\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=lr_warmup_steps,\n",
        "    num_training_steps=max_train_steps,\n",
        "    num_cycles=3\n",
        ")\n",
        "\n",
        "dataset = Text2ImageDataset(\n",
        "    images_folder=images_folder,\n",
        "    captions_folder=captions_folder,\n",
        "    transform=train_transform,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "def collate_fn(examples):\n",
        "    pixel_values = []\n",
        "    input_ids = []\n",
        "    for tensor, input_id in examples:\n",
        "        pixel_values.append(tensor)\n",
        "        input_ids.append(input_id)\n",
        "    pixel_values = torch.stack(pixel_values, dim=0).float()\n",
        "    input_ids = torch.stack(input_ids, dim=0)\n",
        "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    batch_size=train_batch_size,\n",
        "    num_workers=8,\n",
        ")\n",
        "print(\"Preparation Finished!\")"
      ],
      "metadata": {
        "id": "mqSmWcCUOnVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "etTIXiZOUFmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Fine-tuning\n",
        "This cell takes 25 minutes to run in the default setting, but it may vary depending on the condition of Colab and `max_train_step`."
      ],
      "metadata": {
        "id": "jBGv8WXnNDap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "progress_bar = tqdm(\n",
        "    range(0, max_train_steps),\n",
        "    initial=0,\n",
        "    desc=\"Steps\",\n",
        ")\n",
        "global_step = 0\n",
        "num_epochs = math.ceil(max_train_steps / len(train_dataloader))\n",
        "validation_step = int(max_train_steps * validation_step_ratio)\n",
        "best_face_score = float(\"inf\")\n",
        "for epoch in range(num_epochs):\n",
        "    unet.train()\n",
        "    text_encoder.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if global_step >= max_train_steps:\n",
        "            break\n",
        "        latents = vae.encode(batch[\"pixel_values\"].to(DEVICE, dtype=weight_dtype)).latent_dist.sample()\n",
        "        latents = latents * vae.config.scaling_factor\n",
        "        # Sample noise that we'll add to the latents\n",
        "        noise = torch.randn_like(latents)\n",
        "        bsz = latents.shape[0]\n",
        "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
        "        timesteps = timesteps.long()\n",
        "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "        # Get the text embedding for conditioning\n",
        "        encoder_hidden_states = text_encoder(batch[\"input_ids\"].to(latents.device), return_dict=False)[0]\n",
        "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "            target = noise\n",
        "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "            target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
        "        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]\n",
        "        if not snr_gamma:\n",
        "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
        "        else:\n",
        "            snr = compute_snr(noise_scheduler, timesteps)\n",
        "            mse_loss_weights = torch.stack([snr, snr_gamma * torch.ones_like(timesteps)], dim=1).min(\n",
        "                dim=1\n",
        "            )[0]\n",
        "            if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "                mse_loss_weights = mse_loss_weights / snr\n",
        "            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "                mse_loss_weights = mse_loss_weights / (snr + 1)\n",
        "\n",
        "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n",
        "            loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\n",
        "            loss = loss.mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "        global_step += 1\n",
        "\n",
        "        if global_step % validation_step == 0 or global_step == max_train_steps:\n",
        "            save_path = os.path.join(output_folder, f\"checkpoint-last\")\n",
        "            unet_path = os.path.join(save_path, \"unet.pt\")\n",
        "            text_encoder_path = os.path.join(save_path, \"text_encoder.pt\")\n",
        "            print(f\"Saving Checkpoint to {save_path} ......\")\n",
        "            os.makedirs(save_path, exist_ok=True)\n",
        "            torch.save(unet, unet_path)\n",
        "            torch.save(text_encoder, text_encoder_path)\n",
        "            save_path = os.path.join(output_folder, f\"checkpoint-{global_step + 1000}\")\n",
        "            os.makedirs(save_path, exist_ok=True)\n",
        "            face_score, clip_score, mis = evaluate(\n",
        "                pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "                weight_dtype=weight_dtype,\n",
        "                seed=seed,\n",
        "                unet_path=unet_path,\n",
        "                text_encoder_path=text_encoder_path,\n",
        "                validation_prompt=validation_prompt[:validation_prompt_num],\n",
        "                output_folder=save_path,\n",
        "                train_emb=dataset.train_emb\n",
        "            )\n",
        "            print(\"Step:\", global_step, \"Face Similarity Score:\", face_score, \"CLIP Score:\", clip_score, \"Faceless Images:\", mis)\n",
        "            if face_score < best_face_score:\n",
        "                best_face_score = face_score\n",
        "                save_path = os.path.join(output_folder, f\"checkpoint-best\")\n",
        "                unet_path = os.path.join(save_path, \"unet.pt\")\n",
        "                text_encoder_path = os.path.join(save_path, \"text_encoder.pt\")\n",
        "                os.makedirs(save_path, exist_ok=True)\n",
        "                torch.save(unet, unet_path)\n",
        "                torch.save(text_encoder, text_encoder_path)\n",
        "print(\"Fine-tuning Finished!!!\")"
      ],
      "metadata": {
        "id": "TDb7p8O9OqLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÁöÑ ResNet50 Ê®°Âûã\n",
        "resnet = models.resnet50(pretrained=True)\n",
        "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])  # ÁßªÈô§ÊúÄÂêéÁöÑÂÖ®ËøûÊé•Â±Ç\n",
        "resnet = resnet.to(DEVICE)\n",
        "resnet.eval()\n",
        "\n",
        "# ÂõæÂÉèÈ¢ÑÂ§ÑÁêÜ\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# ÊèêÂèñÁâπÂæÅ\n",
        "def extract_features(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = preprocess(image).unsqueeze(0).to(DEVICE)  # Ê∑ªÂä† batch Áª¥Â∫¶\n",
        "    with torch.no_grad():\n",
        "        features = resnet(image)\n",
        "    return features.squeeze().cpu().numpy()  # ÂéªÊéâ batch Áª¥Â∫¶Âπ∂ËΩ¨Êç¢‰∏∫ NumPy Êï∞ÁªÑ\n",
        "\n",
        "# ÊèêÂèñËÆ≠ÁªÉÂõæÂÉèÁöÑÁâπÂæÅ\n",
        "# train_emb = torch.tensor([extract_features(img_path) for img_path in train_image_paths])"
      ],
      "metadata": {
        "id": "bjQOKZ8Am_y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing\n",
        "The fine-tuning process is done. We then want to test our model.\n",
        "\n",
        "We will first load the fine-tuned model for checkpoint we saved and calculate the face similarity, CLIP score, and the number of faceless images.\n",
        "This process will take about 15 minutes."
      ],
      "metadata": {
        "id": "QpZ83eq-NOD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = os.path.join(output_folder, f\"checkpoint-last\") # Ë®≠ÂÆö‰ΩøÁî®Âì™ÂÄãcheckpoint inference\n",
        "unet_path = os.path.join(checkpoint_path, \"unet.pt\")\n",
        "text_encoder_path = os.path.join(checkpoint_path, \"text_encoder.pt\")\n",
        "inference_path = os.path.join(project_dir, \"inference\")\n",
        "os.makedirs(inference_path, exist_ok=True)\n",
        "train_image_paths = []\n",
        "for ext in IMAGE_EXTENSIONS:\n",
        "    train_image_paths.extend(glob.glob(f\"{images_folder}/*{ext}\"))\n",
        "train_image_paths = sorted(train_image_paths)\n",
        "# train_emb = torch.tensor([DeepFace.represent(img_path, detector_backend=\"ssd\", model_name=\"GhostFaceNet\", enforce_detection=False)[0]['embedding'] for img_path in train_image_paths])\n",
        "train_emb = torch.tensor([extract_features(img_path) for img_path in train_image_paths])\n",
        "face_score, clip_score, mis = evaluate(\n",
        "    pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "    weight_dtype=weight_dtype,\n",
        "    seed=seed,\n",
        "    unet_path=unet_path,\n",
        "    text_encoder_path=text_encoder_path,\n",
        "    validation_prompt=validation_prompt,\n",
        "    output_folder=inference_path,\n",
        "    train_emb=train_emb,\n",
        ")\n",
        "print(\"Face Similarity Score:\", face_score, \"CLIP Score:\", clip_score, \"Faceless Images:\", mis)"
      ],
      "metadata": {
        "id": "r0WdWhGnZVnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f0YYxsTva0AN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}