{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "xJcZwF4NM5xx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09459ae3147a447ab97fa103467634ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d128ba9bf4ad45bd8907d7cdc2a28a3a",
              "IPY_MODEL_059d5374355746a49a6371e4ba5f14c6",
              "IPY_MODEL_b5133acffc1e40219ecb30ce0eca064c"
            ],
            "layout": "IPY_MODEL_c3880c0e23ef4e1ea9a39658c6d61421"
          }
        },
        "d128ba9bf4ad45bd8907d7cdc2a28a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ca6bc4f51cc4bfbb38dc9244aa617c4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_66640b68d63742c1a89c3c13d57e14f3",
            "value": "Loading‚Äápipeline‚Äácomponents...:‚Äá100%"
          }
        },
        "059d5374355746a49a6371e4ba5f14c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da2eeed723694387ac9d2607467b5a43",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_103ea5a123294393be9cf98df7ecc0b9",
            "value": 6
          }
        },
        "b5133acffc1e40219ecb30ce0eca064c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ab2e808d1824606aea78831dceb40fc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1cb8d352ea554483b3cdfbb68454d201",
            "value": "‚Äá6/6‚Äá[00:21&lt;00:00,‚Äá‚Äá4.43s/it]"
          }
        },
        "c3880c0e23ef4e1ea9a39658c6d61421": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ca6bc4f51cc4bfbb38dc9244aa617c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66640b68d63742c1a89c3c13d57e14f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da2eeed723694387ac9d2607467b5a43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "103ea5a123294393be9cf98df7ecc0b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ab2e808d1824606aea78831dceb40fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cb8d352ea554483b3cdfbb68454d201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "397db41c022f4b8e81dd3f7312de963d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5277cb5e657429da6b2a58597869b74",
              "IPY_MODEL_11f02a55fe4f4aa7a10c31939032fc01",
              "IPY_MODEL_a1aae041d11a4a5cac5309108e8f2892"
            ],
            "layout": "IPY_MODEL_959d9b32bfff42cfbe7ffdbe62ea042e"
          }
        },
        "a5277cb5e657429da6b2a58597869b74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58121d884d364b2e9474be6051b73bd1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_13cdbe2d4f7b487d880b0a9c337c5b45",
            "value": "100%"
          }
        },
        "11f02a55fe4f4aa7a10c31939032fc01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f91e5efa55f545398b15c87b4dcf59a2",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_692502bf171b4ffc8e3c60cda677c914",
            "value": 30
          }
        },
        "a1aae041d11a4a5cac5309108e8f2892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_238444b8f9c94fb8afaccf58da3935c5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fea7249a15a34045b3060ea9551c6c6d",
            "value": "‚Äá30/30‚Äá[01:31&lt;00:00,‚Äá‚Äá2.93s/it]"
          }
        },
        "959d9b32bfff42cfbe7ffdbe62ea042e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58121d884d364b2e9474be6051b73bd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13cdbe2d4f7b487d880b0a9c337c5b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f91e5efa55f545398b15c87b4dcf59a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "692502bf171b4ffc8e3c60cda677c914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "238444b8f9c94fb8afaccf58da3935c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fea7249a15a34045b3060ea9551c6c6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luoclab/Hung-yi-Lee-HomeWork/blob/main/GenerativeAI2024spring/GenAI_HW10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GenAI HW10: Stable Diffusion Fine-tuning\n",
        "In this homework, you will fine-tune your own Stable Diffusion model to generate your customized images from given text description. For more details, please refer to homework slides\n",
        "\n",
        "## **TODOs**\n",
        "\n",
        "1. Read the slides and make sure you know the objectives of this homework.\n",
        "2. Save a copy of this Colab notebook.\n",
        "3. Follow the steps in this Colab notebook to fine-tune your Stable Diffusion.\n",
        "4. Evaluate outputs using FaceNet and CLIP\n",
        "5. Update results and datasets to NTU COOL\n",
        "If you have any questions, please contact the TAs via TA hours, NTU COOL, or email to ntu-gen-ai-2024-spring-ta@googlegroups.com\n",
        "\n",
        "Tips: At the top of each cell, it shows whether you need to change hyperparameters in that cell and how long it might take.\n",
        "\n",
        "This is based on the work of [Hugging Face](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py).\n",
        "\n",
        "And special thanks to [Celebrity Face Image Dataset](https://www.kaggle.com/datasets/vishesh1412/celebrity-face-image-dataset).\n",
        "\n",
        "\n",
        "Thank you!\n"
      ],
      "metadata": {
        "id": "CnJtiRaRuTFX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNjz10u8biUn",
        "outputId": "0bae956f-cc2f-45b4-ac2d-4fb722536ca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1OXPG2vNb8bG2334HML8vKpqo8UbAEV3d\n",
            "To: /content/drive/MyDrive/GenAI-HW10/Datasets.zip\n",
            "\r  0% 0.00/2.78M [00:00<?, ?B/s]\r 94% 2.62M/2.78M [00:00<00:00, 25.9MB/s]\r100% 2.78M/2.78M [00:00<00:00, 27.2MB/s]\n",
            "fatal: destination path '/content/drive/MyDrive/GenAI-HW10/Brad/logs' already exists and is not an empty directory.\n",
            "‚úÖ Project Brad is ready!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "COLAB = True\n",
        "\n",
        "if COLAB:\n",
        "    from google.colab.output import clear as clear_output\n",
        "else:\n",
        "    from IPython.display import clear_output\n",
        "\n",
        "\n",
        "#@markdown ## Link to Google Drive\n",
        "#@markdown This cell will load some requirements and create the necessary folders in your Google Drive. <p>\n",
        "#@markdown Your project name can't contain spaces but it can contain a single / to make a subfolder in your dataset.\n",
        "project_name = \"Brad\" #@param {type:\"string\"}\n",
        "project_name = project_name.strip()\n",
        "# dataset_name = \"Brad-512\" #@param [\"Brad-512\", \"Anne-512\"]\n",
        "dataset_name = \"Brad\"\n",
        "\n",
        "if not project_name or any(c in project_name for c in \" .()\\\"'\\\\\") or project_name.count(\"/\") > 1:\n",
        "    print(\"Please write a valid project_name.\")\n",
        "else:\n",
        "    if COLAB and not os.path.exists('/content/drive'):\n",
        "      from google.colab import drive\n",
        "      print(\"üìÇ Connecting to Google Drive...\")\n",
        "      drive.mount('/content/drive')\n",
        "\n",
        "    project_base = project_name if \"/\" not in project_name else project_name[:project_name.rfind(\"/\")]\n",
        "    project_subfolder = project_name if \"/\" not in project_name else project_name[project_name.rfind(\"/\")+1:]\n",
        "\n",
        "    root_dir = \"/content\" if COLAB else \"~/Loras\"\n",
        "    main_dir        = os.path.join(root_dir, \"drive/MyDrive/GenAI-HW10\") if COLAB else root_dir\n",
        "    project_dir =  os.path.join(main_dir, project_name)\n",
        "    os.makedirs(main_dir, exist_ok=True)\n",
        "    os.makedirs(project_dir, exist_ok=True)\n",
        "    zip_file = os.path.join(main_dir, \"Datasets.zip\")\n",
        "    !gdown 1OXPG2vNb8bG2334HML8vKpqo8UbAEV3d -O {zip_file}\n",
        "    !unzip -q -o {zip_file} -d {main_dir}\n",
        "    log_file = os.path.join(project_dir, \"logs.zip\")\n",
        "    log_dir = os.path.join(project_dir, \"logs\")\n",
        "    !git clone https://huggingface.co/yahcreeper/GenAI-HW10-Model {log_dir}\n",
        "    # !cd {log_dir}\n",
        "    # !git lfs pull\n",
        "    # !gdown 1kalT3k7kEV0xcD6pf_OTSo7npHmsfZ_z -O {log_file}\n",
        "    # !unzip -q -o {log_file} -d {project_dir}\n",
        "    # !rm -f {log_file}\n",
        "    model_path = os.path.join(project_dir, \"logs\", \"checkpoint-last\")\n",
        "    images_folder   = os.path.join(main_dir, \"Datasets\", dataset_name)\n",
        "    prompts_folder  = os.path.join(main_dir, \"Datasets\", \"prompts\")\n",
        "    captions_folder = images_folder\n",
        "    os.makedirs(images_folder, exist_ok=True)\n",
        "\n",
        "    print(f\"‚úÖ Project {project_name} is ready!\")\n",
        "    step1_installed_flag = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##  Install the required packages\n",
        "#@markdown In this session, we will install some well-established packages to facilitate the fine-tuning process. <p>\n",
        "#@markdown The installation will take about 5 minutes.\n",
        "os.chdir(root_dir)\n",
        "!pip -q install torch==2.5 timm==1.0.7 fairscale==0.4.13 transformers==4.41.2 requests accelerate==0.31.0 diffusers==0.29.1 einop==0.0.1 safetensors==0.4.3 voluptuous==0.15.1 jax peft==0.11.1 deepface==0.0.92 tensorflow==2.15.0 keras==2.15.0"
      ],
      "metadata": {
        "id": "lJ4pB6-_dC91"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##  Import necessary packages\n",
        "#@markdown It is recommmended NOT to change codes in this cell.\n",
        "import argparse\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import glob\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "import transformers\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "from tqdm.auto import tqdm\n",
        "from peft import LoraConfig\n",
        "from peft.utils import get_peft_model_state_dict\n",
        "from transformers import AutoProcessor, AutoModel, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "import diffusers\n",
        "from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, StableDiffusionPipeline, UNet2DConditionModel\n",
        "from diffusers.optimization import get_scheduler\n",
        "from diffusers.utils import convert_state_dict_to_diffusers\n",
        "from diffusers.training_utils import compute_snr\n",
        "from diffusers.utils.torch_utils import is_compiled_module\n",
        "from deepface import DeepFace\n",
        "import cv2"
      ],
      "metadata": {
        "id": "pusB39oYsycq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c34b6290-4c14-4c0a-8f0f-226bd6cd7432"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
            "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change the following parameters, or the process may crashed due to GPU out of memory.\n",
        "output_folder = os.path.join(project_dir, \"logs\") # Â≠òÊîæmodel checkpointsË∑üvalidationÁµêÊûúÁöÑË≥áÊñôÂ§æ\n",
        "seed = 1126 # random seed\n",
        "train_batch_size = 2 # training batch size\n",
        "resolution = 512 # Image size\n",
        "weight_dtype = torch.bfloat16 #\n",
        "snr_gamma = 5\n",
        "#####\n",
        "\n",
        "#@markdown ## Important parameters for fine-tuning Stable Diffusion\n",
        "pretrained_model_name_or_path = \"stablediffusionapi/cyberrealistic-41\"\n",
        "lora_rank = 32\n",
        "lora_alpha = 16\n",
        "#@markdown ### ‚ñ∂Ô∏è Learning Rate\n",
        "#@markdown The learning rate is the most important for your results. If you want to train slower with lots of images, or if your dim and alpha are high, move the unet to 2e-4 or lower. <p>\n",
        "#@markdown The text encoder helps your Lora learn concepts slightly better. It is recommended to make it half or a fifth of the unet. If you're training a style you can even set it to 0.\n",
        "learning_rate = 1e-4 #@param {type:\"number\"}\n",
        "unet_learning_rate = learning_rate\n",
        "text_encoder_learning_rate = learning_rate\n",
        "lr_scheduler_name = \"cosine_with_restarts\" # Ë®≠ÂÆöÂ≠∏ÁøíÁéáÁöÑÊéíÁ®ã\n",
        "lr_warmup_steps = 100 # Ë®≠ÂÆöÁ∑©ÊÖ¢Êõ¥Êñ∞ÁöÑÊ≠•Êï∏\n",
        "#@markdown ### ‚ñ∂Ô∏è Steps\n",
        "#@markdown Choose your training step and the number of generated images per each validaion\n",
        "max_train_steps = 200 #@param {type:\"slider\", min:200, max:2000, step:100}\n",
        "validation_prompt = \"validation_prompt.txt\"\n",
        "validation_prompt_path = os.path.join(prompts_folder, validation_prompt)\n",
        "validation_prompt_num = 3 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "validation_step_ratio = 1 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "with open(validation_prompt_path, \"r\") as f:\n",
        "    validation_prompt = [line.strip() for line in f.readlines()]\n",
        "#####"
      ],
      "metadata": {
        "id": "oTbop_tCsxIb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Some Useful Functions and Class"
      ],
      "metadata": {
        "id": "bpPjMo_IMtt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "IMAGE_EXTENSIONS = [\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".PNG\", \".JPG\", \".JPEG\", \".WEBP\", \".BMP\"]\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "        transforms.CenterCrop(resolution),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "    ]\n",
        ")\n",
        "class Text2ImageDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    (1) Goal:\n",
        "        - This class is used to build dataset for finetuning text-to-image model\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, images_folder, captions_folder, transform, tokenizer):\n",
        "        \"\"\"\n",
        "        (2) Arguments:\n",
        "            - images_folder: str, path to images\n",
        "            - captions_folder: str, path to captions\n",
        "            - transform: function, turn raw image into torch.tensor\n",
        "            - tokenizer: CLIPTokenize, turn sentences into word ids\n",
        "        \"\"\"\n",
        "        self.image_paths = []\n",
        "        for ext in IMAGE_EXTENSIONS:\n",
        "            self.image_paths.extend(glob.glob(f\"{images_folder}/*{ext}\"))\n",
        "        self.image_paths = sorted(self.image_paths)\n",
        "        self.train_emb = torch.tensor([DeepFace.represent(img_path, detector_backend=\"ssd\", model_name=\"GhostFaceNet\", enforce_detection=False)[0]['embedding'] for img_path in self.image_paths])\n",
        "        caption_paths = sorted(glob.glob(f\"{captions_folder}/*txt\"))\n",
        "        captions = []\n",
        "        for p in caption_paths:\n",
        "            with open(p, \"r\") as f:\n",
        "                captions.append(f.readline())\n",
        "        inputs = tokenizer(\n",
        "            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        self.input_ids = inputs.input_ids\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        input_id = self.input_ids[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            # convert to tensor temporarily so dataloader will accept it\n",
        "            tensor = self.transform(image)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load image path: {img_path}, error: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "        return tensor, input_id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "\n",
        "def prepare_lora_model(pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\", model_path=None):\n",
        "    \"\"\"\n",
        "    (1) Goal:\n",
        "        - This function is used to get the whole stable diffusion model with lora layers and freeze non-lora parameters, including Tokenizer, Noise Scheduler, UNet, Text Encoder, and VAE\n",
        "\n",
        "    (2) Arguments:\n",
        "        - pretrained_model_name_or_path: str, model name from Hugging Face\n",
        "        - model_path: str, path to pretrained model.\n",
        "\n",
        "    (3) Returns:\n",
        "        - output: Tokenizer, Noise Scheduler, UNet, Text Encoder, and VAE\n",
        "\n",
        "    \"\"\"\n",
        "    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(\n",
        "        pretrained_model_name_or_path,\n",
        "        subfolder=\"tokenizer\"\n",
        "    )\n",
        "    # text_encoder = CLIPTextModel.from_pretrained(\n",
        "    #     pretrained_model_name_or_path,\n",
        "    #     torch_dtype=weight_dtype,\n",
        "    #     subfolder=\"text_encoder\"\n",
        "    # )\n",
        "    vae = AutoencoderKL.from_pretrained(\n",
        "        pretrained_model_name_or_path,\n",
        "        subfolder=\"vae\"\n",
        "    )\n",
        "    # unet = UNet2DConditionModel.from_pretrained(\n",
        "    #     pretrained_model_name_or_path,\n",
        "    #     torch_dtype=weight_dtype,\n",
        "    #     subfolder=\"unet\"\n",
        "    # )\n",
        "    text_encoder = torch.load(os.path.join(model_path, \"text_encoder.pt\"))\n",
        "    unet = torch.load(os.path.join(model_path, \"unet.pt\"))\n",
        "    vae.requires_grad_(False)\n",
        "    for name, param in unet.named_parameters():\n",
        "        if \"lora\" in name:\n",
        "            param.requires_grad_(True)\n",
        "        else:\n",
        "            param.requires_grad_(False)\n",
        "    for name, param in text_encoder.named_parameters():\n",
        "        if \"lora\" in name:\n",
        "            param.requires_grad_(True)\n",
        "        else:\n",
        "            param.requires_grad_(False)\n",
        "\n",
        "    unet.to(DEVICE, dtype=weight_dtype)\n",
        "    vae.to(DEVICE, dtype=weight_dtype)\n",
        "    text_encoder.to(DEVICE, dtype=weight_dtype)\n",
        "    return tokenizer, noise_scheduler, unet, vae, text_encoder\n",
        "\n",
        "def prepare_optimizer(unet, text_encoder, unet_learning_rate=5e-4, text_encoder_learning_rate=1e-4):\n",
        "    \"\"\"\n",
        "    (1) Goal:\n",
        "        - This function is used to feed trainable parameters from UNet and Text Encoder in to optimizer each with different learning rate\n",
        "\n",
        "    (2) Arguments:\n",
        "        - unet: UNet2DConditionModel, UNet from Hugging Face\n",
        "        - text_encoder: CLIPTextModel, Text Encoder from Hugging Face\n",
        "        - unet_learning_rate: float, learning rate for UNet\n",
        "        - text_encoder_learning_rate: float, learning rate for Text Encoder\n",
        "\n",
        "    (3) Returns:\n",
        "        - output: Optimizer\n",
        "\n",
        "    \"\"\"\n",
        "    unet_lora_layers = list(filter(lambda p: p.requires_grad, unet.parameters()))\n",
        "    text_encoder_lora_layers = list(filter(lambda p: p.requires_grad, text_encoder.parameters()))\n",
        "    trainable_params = [\n",
        "        {\"params\": unet_lora_layers, \"lr\": unet_learning_rate},\n",
        "        {\"params\": text_encoder_lora_layers, \"lr\": text_encoder_learning_rate}\n",
        "    ]\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        trainable_params,\n",
        "        lr=unet_learning_rate,\n",
        "    )\n",
        "    return optimizer\n",
        "\n",
        "def evaluate(pretrained_model_name_or_path, weight_dtype, seed, unet_path, text_encoder_path, validation_prompt, output_folder, train_emb):\n",
        "    \"\"\"\n",
        "    (1) Goal:\n",
        "        - This function is used to evaluate Stable Diffusion by loading UNet and Text Encoder from the given path and calculating face similarity, CLIP score, and the number of faceless images.\n",
        "\n",
        "    (2) Arguments:\n",
        "        - pretrained_model_name_or_path: str, model name from Hugging Face\n",
        "        - weight_dtype: torch.type, model weight type\n",
        "        - seed: int, random seed\n",
        "        - unet_path: str, path to UNet model checkpoint\n",
        "        - text_encoder_path: str, path to Text Encoder model checkpoint\n",
        "        - validation_prompt: list, list of str storing texts for validation\n",
        "        - output_folder: str, directory for saving generated images\n",
        "        - train_emb: tensor, face features of training images\n",
        "\n",
        "    (3) Returns:\n",
        "        - output: face similarity, CLIP score, the number of faceless images\n",
        "\n",
        "    \"\"\"\n",
        "    pipeline = DiffusionPipeline.from_pretrained(\n",
        "        pretrained_model_name_or_path,\n",
        "        torch_dtype=weight_dtype,\n",
        "        safety_checker=None,\n",
        "    )\n",
        "    pipeline.unet = torch.load(unet_path)\n",
        "    pipeline.text_encoder = torch.load(text_encoder_path)\n",
        "    pipeline = pipeline.to(DEVICE)\n",
        "    clip_model_name = \"openai/clip-vit-base-patch32\"\n",
        "    clip_model = AutoModel.from_pretrained(clip_model_name)\n",
        "    clip_processor = AutoProcessor.from_pretrained(clip_model_name)\n",
        "\n",
        "    # run inference\n",
        "    with torch.no_grad():\n",
        "        generator = torch.Generator(device=DEVICE)\n",
        "        generator = generator.manual_seed(seed)\n",
        "        face_score = 0\n",
        "        clip_score = 0\n",
        "        mis = 0\n",
        "        print(\"Generating validaion pictures ......\")\n",
        "        images = []\n",
        "        for i in range(0, len(validation_prompt), 4):\n",
        "            images.extend(pipeline(validation_prompt[i:min(i + 4, len(validation_prompt))], num_inference_steps=30, generator=generator).images)\n",
        "        print(\"Calculating validaion score ......\")\n",
        "        valid_emb = []\n",
        "        for i, image in enumerate(tqdm(images)):\n",
        "            save_file = f\"{output_folder}/valid_image_{i}.png\"\n",
        "            image.save(save_file)\n",
        "            opencvImage = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
        "            emb = DeepFace.represent(\n",
        "                opencvImage,\n",
        "                detector_backend=\"ssd\",\n",
        "                model_name=\"GhostFaceNet\",\n",
        "                enforce_detection=False,\n",
        "            )\n",
        "            if emb == [] or emb[0]['face_confidence'] == 0:\n",
        "                mis += 1\n",
        "                continue\n",
        "            emb = emb[0]\n",
        "            inputs = clip_processor(text=validation_prompt[i], images=image, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                outputs = clip_model(**inputs)\n",
        "            sim = outputs.logits_per_image\n",
        "            clip_score += sim.item()\n",
        "            valid_emb.append(emb['embedding'])\n",
        "        if len(valid_emb) == 0:\n",
        "            return 0, 0, mis\n",
        "        valid_emb = torch.tensor(valid_emb)\n",
        "        valid_emb = (valid_emb / torch.norm(valid_emb, p=2, dim=-1)[:, None]).cuda()\n",
        "        train_emb = (train_emb / torch.norm(train_emb, p=2, dim=-1)[:, None]).cuda()\n",
        "        face_score = torch.cdist(valid_emb, train_emb, p=2).mean().item()\n",
        "        # face_score = torch.min(face_score, 1)[0].mean()\n",
        "        clip_score /= len(validation_prompt) - mis\n",
        "    return face_score, clip_score, mis"
      ],
      "metadata": {
        "id": "3Kuc0_PcHW48"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÁöÑ ResNet Ê®°Âûã\n",
        "resnet = models.resnet50(pretrained=True)\n",
        "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])  # ÂéªÊéâÊúÄÂêéÁöÑÂÖ®ËøûÊé•Â±Ç\n",
        "resnet.eval()\n",
        "\n",
        "def extract_features(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = train_transform(image).unsqueeze(0)  # Ê∑ªÂä† batch Áª¥Â∫¶\n",
        "    with torch.no_grad():\n",
        "        features = resnet(image)\n",
        "    return features.squeeze()  # ÂéªÊéâ batch Áª¥Â∫¶\n",
        "\n",
        "# Âú® Text2ImageDataset ‰∏≠‰ΩøÁî®\n",
        "class Text2ImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images_folder, captions_folder, transform, tokenizer):\n",
        "        self.image_paths = []\n",
        "        for ext in IMAGE_EXTENSIONS:\n",
        "            self.image_paths.extend(glob.glob(f\"{images_folder}/*{ext}\"))\n",
        "        self.image_paths = sorted(self.image_paths)\n",
        "        self.train_emb = torch.stack([extract_features(img_path) for img_path in self.image_paths])\n",
        "        caption_paths = sorted(glob.glob(f\"{captions_folder}/*txt\"))\n",
        "        captions = []\n",
        "        for p in caption_paths:\n",
        "            with open(p, \"r\") as f:\n",
        "                captions.append(f.readline())\n",
        "        inputs = tokenizer(\n",
        "            captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        self.input_ids = inputs.input_ids\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        input_id = self.input_ids[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            tensor = self.transform(image)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load image path: {img_path}, error: {e}\")\n",
        "            return None\n",
        "        return tensor, input_id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)"
      ],
      "metadata": {
        "id": "8Mg_RUOMLbUL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModel, AutoProcessor\n",
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "def evaluate(pretrained_model_name_or_path, weight_dtype, seed, unet_path, text_encoder_path, validation_prompt, output_folder, train_emb):\n",
        "    \"\"\"\n",
        "    (1) Goal:\n",
        "        - This function is used to evaluate Stable Diffusion by loading UNet and Text Encoder from the given path and calculating face similarity, CLIP score, and the number of faceless images.\n",
        "\n",
        "    (2) Arguments:\n",
        "        - pretrained_model_name_or_path: str, model name from Hugging Face\n",
        "        - weight_dtype: torch.type, model weight type\n",
        "        - seed: int, random seed\n",
        "        - unet_path: str, path to UNet model checkpoint\n",
        "        - text_encoder_path: str, path to Text Encoder model checkpoint\n",
        "        - validation_prompt: list, list of str storing texts for validation\n",
        "        - output_folder: str, directory for saving generated images\n",
        "        - train_emb: tensor, face features of training images\n",
        "\n",
        "    (3) Returns:\n",
        "        - output: face similarity, CLIP score, the number of faceless images\n",
        "    \"\"\"\n",
        "    # Load ResNet50 model for feature extraction\n",
        "    resnet = models.resnet50(pretrained=True)\n",
        "    resnet = torch.nn.Sequential(*list(resnet.children())[:-1])  # Remove the final classification layer\n",
        "    resnet = resnet.to(DEVICE)\n",
        "    resnet.eval()\n",
        "\n",
        "    # Image preprocessing for ResNet50\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    # Load Stable Diffusion pipeline\n",
        "    pipeline = DiffusionPipeline.from_pretrained(\n",
        "        pretrained_model_name_or_path,\n",
        "        torch_dtype=weight_dtype,\n",
        "        safety_checker=None,\n",
        "    )\n",
        "    pipeline.unet = torch.load(unet_path)\n",
        "    pipeline.text_encoder = torch.load(text_encoder_path)\n",
        "    pipeline = pipeline.to(DEVICE)\n",
        "\n",
        "    # Load CLIP model for CLIP score\n",
        "    clip_model_name = \"openai/clip-vit-base-patch32\"\n",
        "    clip_model = AutoModel.from_pretrained(clip_model_name)\n",
        "    clip_processor = AutoProcessor.from_pretrained(clip_model_name)\n",
        "\n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        generator = torch.Generator(device=DEVICE)\n",
        "        generator = generator.manual_seed(seed)\n",
        "        face_score = 0\n",
        "        clip_score = 0\n",
        "        mis = 0\n",
        "        print(\"Generating validation pictures ......\")\n",
        "        images = []\n",
        "        for i in range(0, len(validation_prompt), 4):\n",
        "            images.extend(pipeline(validation_prompt[i:min(i + 4, len(validation_prompt))], num_inference_steps=30, generator=generator).images)\n",
        "        print(\"Calculating validation score ......\")\n",
        "        valid_emb = []\n",
        "        for i, image in enumerate(tqdm(images)):\n",
        "            save_file = f\"{output_folder}/valid_image_{i}.png\"\n",
        "            image.save(save_file)\n",
        "\n",
        "            # Preprocess image for ResNet50\n",
        "            image_tensor = preprocess(image).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "            # Extract features using ResNet50\n",
        "            with torch.no_grad():\n",
        "                emb = resnet(image_tensor).squeeze().cpu().numpy()\n",
        "\n",
        "            # Check if the image contains a face\n",
        "            if np.linalg.norm(emb) < 1e-6:  # Simple threshold to detect faceless images\n",
        "                mis += 1\n",
        "                continue\n",
        "\n",
        "            # Calculate CLIP score\n",
        "            inputs = clip_processor(text=validation_prompt[i], images=image, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                outputs = clip_model(**inputs)\n",
        "            sim = outputs.logits_per_image\n",
        "            clip_score += sim.item()\n",
        "\n",
        "            # Store valid embeddings\n",
        "            valid_emb.append(emb)\n",
        "\n",
        "        if len(valid_emb) == 0:\n",
        "            return 0, 0, mis\n",
        "\n",
        "        # Normalize embeddings\n",
        "        valid_emb = torch.tensor(valid_emb)\n",
        "        valid_emb = (valid_emb / torch.norm(valid_emb, p=2, dim=-1)[:, None]).to(DEVICE)\n",
        "        train_emb = (train_emb / torch.norm(train_emb, p=2, dim=-1)[:, None]).to(DEVICE)\n",
        "\n",
        "        # Calculate face similarity\n",
        "        face_score = torch.cdist(valid_emb, train_emb, p=2).mean().item()\n",
        "        clip_score /= len(validation_prompt) - mis\n",
        "\n",
        "    return face_score, clip_score, mis"
      ],
      "metadata": {
        "id": "3PtZMrzRSNL1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Dataset, LoRA model, and Optimizer\n",
        "Declare everything needed for Stable Diffusion fine-tuning."
      ],
      "metadata": {
        "id": "xJcZwF4NM5xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, noise_scheduler, unet, vae, text_encoder = prepare_lora_model(pretrained_model_name_or_path, model_path)\n",
        "optimizer                                           = prepare_optimizer(unet, text_encoder, unet_learning_rate, text_encoder_learning_rate)\n",
        "lr_scheduler = get_scheduler(\n",
        "    lr_scheduler_name,\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=lr_warmup_steps,\n",
        "    num_training_steps=max_train_steps,\n",
        "    num_cycles=3\n",
        ")\n",
        "\n",
        "dataset = Text2ImageDataset(\n",
        "    images_folder=images_folder,\n",
        "    captions_folder=captions_folder,\n",
        "    transform=train_transform,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "def collate_fn(examples):\n",
        "    pixel_values = []\n",
        "    input_ids = []\n",
        "    for tensor, input_id in examples:\n",
        "        pixel_values.append(tensor)\n",
        "        input_ids.append(input_id)\n",
        "    pixel_values = torch.stack(pixel_values, dim=0).float()\n",
        "    input_ids = torch.stack(input_ids, dim=0)\n",
        "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    batch_size=train_batch_size,\n",
        "    num_workers=8,\n",
        ")\n",
        "print(\"Preparation Finished!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqSmWcCUOnVb",
        "outputId": "41ea0f9d-6bff-4c8f-ff98-dda6b4729d63"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "An error occurred while trying to fetch stablediffusionapi/cyberrealistic-41: stablediffusionapi/cyberrealistic-41 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
            "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparation Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "etTIXiZOUFmp",
        "outputId": "a53a0cfb-620a-4c25-a9e6-96ad9ab70bef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar 25 03:23:14 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0             31W /   70W |    2226MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Fine-tuning\n",
        "This cell takes 25 minutes to run in the default setting, but it may vary depending on the condition of Colab and `max_train_step`."
      ],
      "metadata": {
        "id": "jBGv8WXnNDap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "progress_bar = tqdm(\n",
        "    range(0, max_train_steps),\n",
        "    initial=0,\n",
        "    desc=\"Steps\",\n",
        ")\n",
        "global_step = 0\n",
        "num_epochs = math.ceil(max_train_steps / len(train_dataloader))\n",
        "validation_step = int(max_train_steps * validation_step_ratio)\n",
        "best_face_score = float(\"inf\")\n",
        "for epoch in range(num_epochs):\n",
        "    unet.train()\n",
        "    text_encoder.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if global_step >= max_train_steps:\n",
        "            break\n",
        "        latents = vae.encode(batch[\"pixel_values\"].to(DEVICE, dtype=weight_dtype)).latent_dist.sample()\n",
        "        latents = latents * vae.config.scaling_factor\n",
        "        # Sample noise that we'll add to the latents\n",
        "        noise = torch.randn_like(latents)\n",
        "        bsz = latents.shape[0]\n",
        "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
        "        timesteps = timesteps.long()\n",
        "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "        # Get the text embedding for conditioning\n",
        "        encoder_hidden_states = text_encoder(batch[\"input_ids\"].to(latents.device), return_dict=False)[0]\n",
        "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "            target = noise\n",
        "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "            target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
        "        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]\n",
        "        if not snr_gamma:\n",
        "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
        "        else:\n",
        "            snr = compute_snr(noise_scheduler, timesteps)\n",
        "            mse_loss_weights = torch.stack([snr, snr_gamma * torch.ones_like(timesteps)], dim=1).min(\n",
        "                dim=1\n",
        "            )[0]\n",
        "            if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "                mse_loss_weights = mse_loss_weights / snr\n",
        "            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "                mse_loss_weights = mse_loss_weights / (snr + 1)\n",
        "\n",
        "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n",
        "            loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\n",
        "            loss = loss.mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "        global_step += 1\n",
        "\n",
        "        if global_step % validation_step == 0 or global_step == max_train_steps:\n",
        "            save_path = os.path.join(output_folder, f\"checkpoint-last\")\n",
        "            unet_path = os.path.join(save_path, \"unet.pt\")\n",
        "            text_encoder_path = os.path.join(save_path, \"text_encoder.pt\")\n",
        "            print(f\"Saving Checkpoint to {save_path} ......\")\n",
        "            os.makedirs(save_path, exist_ok=True)\n",
        "            torch.save(unet, unet_path)\n",
        "            torch.save(text_encoder, text_encoder_path)\n",
        "            save_path = os.path.join(output_folder, f\"checkpoint-{global_step + 1000}\")\n",
        "            os.makedirs(save_path, exist_ok=True)\n",
        "            face_score, clip_score, mis = evaluate(\n",
        "                pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "                weight_dtype=weight_dtype,\n",
        "                seed=seed,\n",
        "                unet_path=unet_path,\n",
        "                text_encoder_path=text_encoder_path,\n",
        "                validation_prompt=validation_prompt[:validation_prompt_num],\n",
        "                output_folder=save_path,\n",
        "                train_emb=dataset.train_emb\n",
        "            )\n",
        "            print(\"Step:\", global_step, \"Face Similarity Score:\", face_score, \"CLIP Score:\", clip_score, \"Faceless Images:\", mis)\n",
        "            if face_score < best_face_score:\n",
        "                best_face_score = face_score\n",
        "                save_path = os.path.join(output_folder, f\"checkpoint-best\")\n",
        "                unet_path = os.path.join(save_path, \"unet.pt\")\n",
        "                text_encoder_path = os.path.join(save_path, \"text_encoder.pt\")\n",
        "                os.makedirs(save_path, exist_ok=True)\n",
        "                torch.save(unet, unet_path)\n",
        "                torch.save(text_encoder, text_encoder_path)\n",
        "print(\"Fine-tuning Finished!!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379,
          "referenced_widgets": [
            "09459ae3147a447ab97fa103467634ac",
            "d128ba9bf4ad45bd8907d7cdc2a28a3a",
            "059d5374355746a49a6371e4ba5f14c6",
            "b5133acffc1e40219ecb30ce0eca064c",
            "c3880c0e23ef4e1ea9a39658c6d61421",
            "6ca6bc4f51cc4bfbb38dc9244aa617c4",
            "66640b68d63742c1a89c3c13d57e14f3",
            "da2eeed723694387ac9d2607467b5a43",
            "103ea5a123294393be9cf98df7ecc0b9",
            "3ab2e808d1824606aea78831dceb40fc",
            "1cb8d352ea554483b3cdfbb68454d201",
            "397db41c022f4b8e81dd3f7312de963d",
            "a5277cb5e657429da6b2a58597869b74",
            "11f02a55fe4f4aa7a10c31939032fc01",
            "a1aae041d11a4a5cac5309108e8f2892",
            "959d9b32bfff42cfbe7ffdbe62ea042e",
            "58121d884d364b2e9474be6051b73bd1",
            "13cdbe2d4f7b487d880b0a9c337c5b45",
            "f91e5efa55f545398b15c87b4dcf59a2",
            "692502bf171b4ffc8e3c60cda677c914",
            "238444b8f9c94fb8afaccf58da3935c5",
            "fea7249a15a34045b3060ea9551c6c6d"
          ]
        },
        "id": "TDb7p8O9OqLM",
        "outputId": "fafd741b-dca2-49d7-ecac-2caa5ae70ef0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Steps: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [11:00<00:00,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Checkpoint to /content/drive/MyDrive/GenAI-HW10/Brad/logs/checkpoint-last ......\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vae/diffusion_pytorch_model.safetensors not found\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09459ae3147a447ab97fa103467634ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "An error occurred while trying to fetch /root/.cache/huggingface/hub/models--stablediffusionapi--cyberrealistic-41/snapshots/f996a745631c2bf8c07ca1b40d06cadd59f03f53/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /root/.cache/huggingface/hub/models--stablediffusionapi--cyberrealistic-41/snapshots/f996a745631c2bf8c07ca1b40d06cadd59f03f53/vae.\n",
            "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
            "An error occurred while trying to fetch /root/.cache/huggingface/hub/models--stablediffusionapi--cyberrealistic-41/snapshots/f996a745631c2bf8c07ca1b40d06cadd59f03f53/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /root/.cache/huggingface/hub/models--stablediffusionapi--cyberrealistic-41/snapshots/f996a745631c2bf8c07ca1b40d06cadd59f03f53/unet.\n",
            "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating validation pictures ......\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "397db41c022f4b8e81dd3f7312de963d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating validation score ......\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:02,  1.30s/it]\u001b[A\n",
            " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:01<00:00,  1.35it/s]\u001b[A\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 200 Face Similarity Score: 0.7635918855667114 CLIP Score: 29.558557510375977 Faceless Images: 0\n",
            "Fine-tuning Finished!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing\n",
        "The fine-tuning process is done. We then want to test our model.\n",
        "\n",
        "We will first load the fine-tuned model for checkpoint we saved and calculate the face similarity, CLIP score, and the number of faceless images.\n",
        "This process will take about 15 minutes."
      ],
      "metadata": {
        "id": "QpZ83eq-NOD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = os.path.join(output_folder, f\"checkpoint-last\") # Ë®≠ÂÆö‰ΩøÁî®Âì™ÂÄãcheckpoint inference\n",
        "unet_path = os.path.join(checkpoint_path, \"unet.pt\")\n",
        "text_encoder_path = os.path.join(checkpoint_path, \"text_encoder.pt\")\n",
        "inference_path = os.path.join(project_dir, \"inference\")\n",
        "os.makedirs(inference_path, exist_ok=True)\n",
        "train_image_paths = []\n",
        "for ext in IMAGE_EXTENSIONS:\n",
        "    train_image_paths.extend(glob.glob(f\"{images_folder}/*{ext}\"))\n",
        "train_image_paths = sorted(train_image_paths)\n",
        "train_emb = torch.tensor([DeepFace.represent(img_path, detector_backend=\"ssd\", model_name=\"GhostFaceNet\", enforce_detection=False)[0]['embedding'] for img_path in train_image_paths])\n",
        "\n",
        "face_score, clip_score, mis = evaluate(\n",
        "    pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "    weight_dtype=weight_dtype,\n",
        "    seed=seed,\n",
        "    unet_path=unet_path,\n",
        "    text_encoder_path=text_encoder_path,\n",
        "    validation_prompt=validation_prompt,\n",
        "    output_folder=inference_path,\n",
        "    train_emb=train_emb,\n",
        ")\n",
        "print(\"Face Similarity Score:\", face_score, \"CLIP Score:\", clip_score, \"Faceless Images:\", mis)"
      ],
      "metadata": {
        "id": "r0WdWhGnZVnz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "e886f87d-92d6-45fb-da37-ac4244154222"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Layer count mismatch when loading weights from file. Model expected 178 layers, found 219 saved layers.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-07d8e9631d52>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_image_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{images_folder}/*{ext}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_image_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_image_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDeepFace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_backend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ssd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GhostFaceNet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_detection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_image_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m face_score, clip_score, mis = evaluate(\n",
            "\u001b[0;32m<ipython-input-11-07d8e9631d52>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_image_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{images_folder}/*{ext}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_image_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_image_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDeepFace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_backend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ssd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GhostFaceNet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_detection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_image_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m face_score, clip_score, mis = evaluate(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepface/DeepFace.py\u001b[0m in \u001b[0;36mrepresent\u001b[0;34m(img_path, model_name, enforce_detection, detector_backend, align, expand_percentage, normalization, anti_spoofing)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mto\u001b[0m \u001b[0;34m'skip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mconfidence\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnonsensical\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \"\"\"\n\u001b[0;32m--> 409\u001b[0;31m     return representation.represent(\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0mimg_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepface/modules/representation.py\u001b[0m in \u001b[0;36mrepresent\u001b[0;34m(img_path, model_name, enforce_detection, detector_backend, align, expand_percentage, normalization, anti_spoofing)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mresp_objs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFacialRecognition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# ---------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepface/modules/modeling.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mmodel_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid model_name passed - {model_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepface/basemodels/GhostFaceNet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m112\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m112\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepface/basemodels/GhostFaceNet.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Pre-trained weights is just downloaded to {output}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/legacy/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, model)\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0mlayer_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_layer_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;34m\"Layer count mismatch when loading weights from file. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;34mf\"Model expected {len(filtered_layers)} layers, found \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Layer count mismatch when loading weights from file. Model expected 178 layers, found 219 saved layers."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f0YYxsTva0AN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}